{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARS-CoV-2 data processing\n",
    "\n",
    "This notebook performs four main functions:\n",
    "\n",
    "- [Merging sequences and metadata](#merge)  \n",
    "- [Selecting and exporting sequences for analysis](#select)  \n",
    "- [Finding groups of linked mutations and classifying mutations as synonymous or nonsynonymous](#link)\n",
    "- [Inferring selection coefficients for various different scenarios](#infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version 3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n",
      "numpy version 1.18.5\n",
      "pandas version 1.0.5\n",
      "matplotlib version 3.2.2\n",
      "scipy version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from imp import reload\n",
    "import datetime as dt\n",
    "import shutil\n",
    "print('python version %s' % sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print('numpy version %s' % np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas version %s' % pd.__version__)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib version %s' % matplotlib.__version__)\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "print('scipy version %s' % scipy.__version__)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "\n",
    "# sequence related variables\n",
    "NUC = ['-', 'A', 'C', 'G', 'T']\n",
    "\n",
    "# local directories\n",
    "SARS_DIR     = os.getcwd()\n",
    "#INF_SCRIPTS  = 'inf-scripts'\n",
    "DATA_DATE    = '2021-08-14'\n",
    "DATA_DIR     = os.path.join(SARS_DIR, 'data')\n",
    "SCRIPT_DIR   = os.path.join(SARS_DIR, 'processing-files')\n",
    "INF_SCRIPTS  = SCRIPT_DIR\n",
    "INF_DIR      = DATA_DIR\n",
    "\n",
    "METADATA_COMP    = os.path.join(SARS_DIR, f'metadata_tsv_2021_12_06.tar.xz')          # compressed metadata file from GISAID\n",
    "METADATA_FILE    = os.path.join(SARS_DIR, f'metadata-{DATA_DATE}', 'metadata.tsv')    # decompressed metadata file from GISAID\n",
    "METADATA_DIR     = os.path.join(SARS_DIR, f'metadata-{DATA_DATE}')\n",
    "\n",
    "METADATA_INTERIM = 'merged-interim.csv'\n",
    "MERGED_FINAL     = 'merged-final.csv'\n",
    "\n",
    "# cluster directories\n",
    "USER_NAME  = 'blee098'\n",
    "EMAIL_EXT  = '@ucr.edu'    # email extension for account attached to cluster\n",
    "SSH_HOME   = 'blee098@cluster.hpcc.ucr.edu:'\n",
    "SSH_DATA   = os.path.join('/rhome', USER_NAME, 'bigdata', 'SARS-CoV-2-Data')\n",
    "SCRATCH    = os.path.join(SSH_DATA, 'scratch')\n",
    "\n",
    "# metadata variables\n",
    "METADATA_COLS    = [   'accession', 'virus_name',            'date', 'location',             'location_additional']\n",
    "METADATA_OCOLS   = ['Accession ID', 'Virus name', 'Collection date', 'Location', 'Additional location information']\n",
    "METADATA_XFORM   = [           str,    str.lower,               str,  str.lower,                         str.lower]\n",
    "REF_TAG          = 'EPI_ISL_402125'\n",
    "\n",
    "# SEQUENCE PROCESSING GLOBAL VARIABLES\n",
    "\n",
    "START_IDX    = 0\n",
    "END_IDX      = -1\n",
    "MAX_GAP_NUM  = 20000\n",
    "MAX_GAP_FREQ = 0.99\n",
    "MIN_SEQS     = 0\n",
    "MAX_DT       = 999\n",
    "\n",
    "import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for directory in [INF_DIR, DATA_DATE, SARS_DIR, INF_SCRIPTS, DATA_DIR, SCRIPT_DIR]:\n",
    "for directory in [INF_DIR, SARS_DIR, INF_SCRIPTS, DATA_DIR, SCRIPT_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merge'></a>\n",
    "# Merging sequences and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the accessions, virus name, date, and location information from metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mkdir {METADATA_DIR} &&')\n",
    "print(f'tar -xf {METADATA_COMP} -C {METADATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-b092cfb12bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m use_cols = ['Virus name', 'Accession ID', 'Collection date', 'Location', \n\u001b[1;32m      5\u001b[0m             'Additional location information', 'Host']\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mMETADATA_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mMETADATA_INTERIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accessions = set()\n",
    "header     = ','.join(METADATA_COLS)\n",
    "\n",
    "use_cols = ['Accession ID', 'Virus name', 'Collection date', 'Location', \n",
    "            'Additional location information', 'Host']\n",
    "df = pd.read_csv('%s' % METADATA_FILE, sep='\\t', usecols=use_cols)\n",
    "f = open('%s' % METADATA_INTERIM, 'w')\n",
    "f.write('%s\\n' % header)\n",
    "for df_iter, df_entry in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    #if df_iter % 10000 == 0:\n",
    "    #    print(df_inter / len(df))\n",
    "    entry = []\n",
    "    unique_id  = False\n",
    "    valid_date = False\n",
    "        \n",
    "    # check that host is human\n",
    "    if 'Host' in df_entry:\n",
    "        if df_entry.Host!='Human' and df_entry.Host!='human':\n",
    "            continue\n",
    "        \n",
    "    # get accession\n",
    "    acc_idx = METADATA_COLS.index('accession')\n",
    "    if 'Accession ID' not in df_entry:\n",
    "        continue\n",
    "    xformed = METADATA_XFORM[acc_idx](df_entry[METADATA_OCOLS[acc_idx]])\n",
    "    acc     = METADATA_XFORM[acc_idx](df_entry[METADATA_OCOLS[acc_idx]])\n",
    "\n",
    "    if xformed not in accessions:\n",
    "        #accessions.append(xformed)\n",
    "        accessions.add(xformed)\n",
    "        entry.append(xformed)\n",
    "        unique_id = True\n",
    "    #else:\n",
    "        #print('Excluding %s (duplicate accession number)' % xformed)\n",
    "        \n",
    "    # get virus name \n",
    "    if 'Virus name' in df_entry:\n",
    "        name_idx = METADATA_COLS.index('virus_name')\n",
    "        xformed  = METADATA_XFORM[name_idx](df_entry[METADATA_OCOLS[name_idx]])\n",
    "        xformed  = xformed.replace(',', ' ')\n",
    "        entry.append(xformed)\n",
    "    else:\n",
    "        entry.append('NA')\n",
    "            \n",
    "    # get date\n",
    "    if unique_id:\n",
    "        date_idx = METADATA_COLS.index('date')\n",
    "        xformed  = METADATA_XFORM[date_idx](df_entry[METADATA_OCOLS[date_idx]])\n",
    "            \n",
    "        try:\n",
    "            dt.date.fromisoformat(xformed)\n",
    "            valid_date = True\n",
    "        except:\n",
    "            valid_date = False\n",
    "            #print('Excluding %s (date %s is incomplete)' % (entry[0], xformed))\n",
    "        entry.append(xformed)\n",
    "            \n",
    "    # get location \n",
    "    if unique_id and valid_date:\n",
    "        loc_idx = METADATA_COLS.index('location')\n",
    "        xformed = METADATA_XFORM[loc_idx](df_entry[METADATA_OCOLS[loc_idx]])\n",
    "        xformed = xformed.replace(',', ' ')\n",
    "        entry.append(xformed)\n",
    "        \n",
    "        loc_add_idx = METADATA_COLS.index('location_additional')\n",
    "        xformed     = METADATA_XFORM[loc_add_idx](str(df_entry[METADATA_OCOLS[loc_add_idx]]))\n",
    "        xformed     = xformed.replace(',', ' ')\n",
    "        entry.append(xformed)\n",
    "        \n",
    "    # save data\n",
    "    if unique_id and valid_date:\n",
    "        f.write('%s\\n' % ','.join(entry))\n",
    "            \n",
    "\n",
    "# Add reference if not in the list of accessions\n",
    "if REF_TAG not in accessions:\n",
    "    entry = []\n",
    "    for i in range(len(METADATA_COLS)):\n",
    "        if METADATA_COLS[i]=='accession':\n",
    "            entry.append(REF_TAG)\n",
    "        elif METADATA_COLS[i]=='date':\n",
    "            entry.append('2020-01-01')\n",
    "        else:\n",
    "            entry.append('reference')\n",
    "    f.write('%s\\n' % ','.join(entry))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging sequence data in the GISAID alignment with metadata on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/merge-metadata-alignment.py blee098@cluster.hpcc.ucr.edu: &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/merged-interim.csv blee098@cluster.hpcc.ucr.edu://rhome/blee098/bigdata/SARS-CoV-2-Data &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu: &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/job-merge-alignment.sh blee098@cluster.hpcc.ucr.edu: \n",
      "\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/msa_0814.tar.xz blee098@cluster.hpcc.ucr.edu://rhome/blee098/bigdata/SARS-CoV-2-Data/msa_0814.tar.xz\n",
      "\n",
      "tar -xf /rhome/blee098/bigdata/SARS-CoV-2-Data/msa_0814.tar.xz -C /rhome/blee098/bigdata/SARS-CoV-2-Data &&\n",
      "mkdir -p ./MPL/out &&\n",
      "sbatch job-merge-alignment.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#np.save(os.path.join(SARS_DIR, 'date-ranges.npy'), DATE_RANGES)\n",
    "msa_name         = 'msa_0814'\n",
    "msa_file         = f'{msa_name}.fasta'\n",
    "msa_path_cluster = os.path.join(SSH_DATA, msa_name, msa_name + '.fasta')\n",
    "script_file      = 'merge-metadata-alignment.py'\n",
    "job_file         = 'job-merge-alignment.sh'\n",
    "job_str          = f\"\"\"#!/bin/bash\n",
    "\n",
    "#SBATCH --time=4-0\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --job-name=merge\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH -e ./MPL/out/merge-error\n",
    "#SBATCH -o ./MPL/out/merge-out\n",
    "#SBATCH -p highmem\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --meta_file {os.path.join(SSH_DATA, METADATA_INTERIM)} --msa_file {msa_path_cluster} -o {os.path.join(SSH_DATA, MERGED_FINAL[:MERGED_FINAL.find('.')])}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SARS_DIR, job_file), mode='w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "print('scp %s/%s %s &&' % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s/%s &&' % (SARS_DIR, METADATA_INTERIM, SSH_HOME, SSH_DATA))\n",
    "print('scp %s/%s %s &&' % (SCRIPT_DIR, 'data_processing.py', SSH_HOME))\n",
    "print('scp %s/%s %s '   % (SARS_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "msa_file2 = f'{msa_name}.tar.xz'\n",
    "msa_path2 = os.path.join(SARS_DIR, msa_file2)\n",
    "print('scp %s %s/%s/%s' % (msa_path2, SSH_HOME, SSH_DATA, msa_file2))\n",
    "print('')\n",
    "\n",
    "# commands to execute on the cluster\n",
    "print('tar -xf %s/%s -C %s &&' % (SSH_DATA, msa_file2, SSH_DATA))\n",
    "print('mkdir -p ./MPL/out &&')\n",
    "print('sbatch %s' % job_file)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking sampling by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/check-regional.py blee098@cluster.hpcc.ucr.edu: &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-check-regional.sh blee098@cluster.hpcc.ucr.edu:\n",
      "\n",
      "sbatch job-check-regional.sh\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-usa.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-usa-sub.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-uk.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-uk-reg.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-england.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-scotland.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-wales.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-brazil.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-canada.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-brazil-sub.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-canada-sub.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-turkey.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-russia.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-russia-sub.csv /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/regional-2021-08-14-india.csv /Users/brianlee/SARS-CoV-2-Data\n"
     ]
    }
   ],
   "source": [
    "# finding the number of sequences in each region\n",
    "\n",
    "out_file      = f'regional-{DATA_DATE}'\n",
    "script_file   = 'check-regional.py'\n",
    "job_dir       = SSH_HOME + SSH_DATA\n",
    "job_file      = 'job-check-regional.sh'\n",
    "sars_dir_loc  = './bigdata/SARS-CoV-2-Data/'\n",
    "job_str       = f\"\"\"#!/bin/bash\n",
    "\n",
    "#SBATCH --time=0-3\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem-per-cpu=100G\n",
    "#SBATCH --job-name=regional-check\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH -e ./MPL/out/region-error\n",
    "#SBATCH -o ./MPL/out/region-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --alignment {os.path.join(SSH_DATA, MERGED_FINAL)} -o {os.path.join(SSH_DATA, out_file)}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), mode='w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "# transfer data processing scripts and job file to the cluster\n",
    "print('scp %s/%s %s &&' % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s' % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run the job on the cluster\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "\n",
    "# transfer output data back to local directory\n",
    "print('scp %s/%s %s &&' % (job_dir, out_file+'.csv', SARS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(SARS_DIR, f'regional-{DATA_DATE}.csv'))\n",
    "print(df['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa = df[df['country']=='united states']\n",
    "print(df_usa['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting regions and time series for analysis\n",
    "<a id='select'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing regions to extract sequence data for\n",
    "\n",
    "identifier = f'regions-{DATA_DATE}'\n",
    "\n",
    "selected   =  [['south america', 'peru', None, None, None, None],\n",
    "               ['north america', 'usa', 'alabama', None, None, None],\n",
    "               ['north america', 'usa', 'arizona', None, None, None],\n",
    "               ['north america', 'usa', 'arkansas', None, None, None],\n",
    "               ['north america', 'usa', 'california',\n",
    "                ['los angeles', 'los angeles county', 'orange', 'orange county', 'san diego', 'san diego county', \n",
    "                 'ventura', 'ventura county', 'san luis obispo', 'san luis obispo county', 'imperial', 'imperial county'], \n",
    "                None, None],\n",
    "               ['north america', 'usa', 'colorado', None, None, None],\n",
    "               ['north america', 'usa', 'connecticut', None, None, None],\n",
    "               ['north america', 'usa', 'delaware', None, None, None],\n",
    "               ['north america', 'usa', 'district of columbia', None, None, None],\n",
    "               ['north america', 'usa', 'florida', None, None, None],\n",
    "               ['north america', 'usa', 'georgia', None, None, None],\n",
    "               ['north america', 'usa', 'hawaii', None, None, None],\n",
    "               ['north america', 'usa', 'idaho', None, None, None],\n",
    "               ['north america', 'usa', 'illinois', None, None, None],\n",
    "               ['north america', 'usa', 'indiana', None, None, None],\n",
    "               ['north america', 'usa', 'kansas', None, None, None],\n",
    "               ['north america', 'usa', 'kentucky', None, None, None],\n",
    "               ['north america', 'usa', 'louisiana', None, None, None],\n",
    "               ['north america', 'usa', 'maine', None, None, None],\n",
    "               ['north america', 'usa', 'maryland', None, None, None], \n",
    "               ['north america', 'usa', 'massachusetts', None, None, None],\n",
    "               ['north america', 'usa', 'michigan', None, None, None],\n",
    "               ['north america', 'usa', 'minnesota', None, None, None],\n",
    "               ['north america', 'usa', 'missouri', None, None, None],\n",
    "               ['north america', 'usa', 'montana', None, None, None],\n",
    "               ['north america', 'usa', 'nebraska', None, None, None],\n",
    "               ['north america', 'usa', 'nevada', None, None, None],\n",
    "               ['north america', 'usa', 'new hampshire', None, None, None],\n",
    "               ['north america', 'usa', 'new jersey', None, None, None],\n",
    "               ['north america', 'usa', 'new mexico', None, None, None],\n",
    "               ['north america', 'usa', 'new york', None, None, None],\n",
    "               ['north america', 'usa', 'north carolina', None, None, None],\n",
    "               ['north america', 'usa', 'north dakota', None, None, None],\n",
    "               ['north america', 'usa', 'ohio', None, None, None],\n",
    "               ['north america', 'usa', 'oregon', None, None, None], \n",
    "               ['north america', 'usa', 'pennsylvania', None, None, None],\n",
    "               ['north america', 'usa', 'puerto rico', None, None, None],\n",
    "               ['north america', 'usa', 'rhode island', None, None, None],\n",
    "               ['north america', 'usa', 'south carolina', None, None, None],\n",
    "               ['north america', 'usa', 'tennessee', None, None, None],\n",
    "               ['north america', 'usa', 'texas', None, None, None],\n",
    "               ['north america', 'usa', 'utah', None, None, None],\n",
    "               ['north america', 'usa', 'virginia', None, None, None],\n",
    "               ['north america', 'usa', 'washington', None, None, None], \n",
    "               ['north america', 'usa', 'west virginia', None, None, None],\n",
    "               ['north america', 'usa', 'wisconsin', None, None, None],\n",
    "               ['north america', 'usa', 'wyoming', None, None, None],\n",
    "               ['europe', 'austria', None, None, None, None],\n",
    "               ['europe', 'bangladesh', None, None, None, None],\n",
    "               ['europe', 'belgium', None, None, None, None],\n",
    "               ['europe', 'bulgaria', None, None, None, None],\n",
    "               ['europe', 'czech republic', None, None, None, None], \n",
    "               ['europe', 'denmark', None, None, None, None],\n",
    "               ['europe', 'estonia', None, None, None, None],\n",
    "               ['europe', 'finland', None, None, None, None],\n",
    "               ['europe', 'france', None, None, None, None], \n",
    "               ['europe', 'germany', None, None, None, None],\n",
    "               ['europe', 'greece', None, None, None, None],\n",
    "               ['europe', 'ireland', None, None, None, None],\n",
    "               ['europe', 'israel', None, None, None, None],\n",
    "               ['europe', 'italy', None, None, None, None],\n",
    "               ['europe', 'jordan', None, None, None, None],\n",
    "               ['europe', 'latvia', None, None, None, None],\n",
    "               ['europe', 'lithuania', None, None, None, None], \n",
    "               ['europe', 'luxembourg', None, None, None, None],\n",
    "               ['europe', 'netherlands', None, None, None, None],\n",
    "               ['europe', 'norway', None, None, None, None],\n",
    "               ['europe', 'poland', None, None, None, None], \n",
    "               ['europe', 'portugal', None, None, None, None],\n",
    "               ['europe', 'romania', None, None, None, None],\n",
    "               ['europe', 'slovakia', None, None, None, None],\n",
    "               ['europe', 'slovenia', None, None, None, None],\n",
    "               ['europe', 'spain', None, None, None, None],\n",
    "               ['europe', 'sweden', None, None, None, None], \n",
    "               ['europe', 'switzerland', None, None, None, None],\n",
    "               ['europe', 'united arab emirates', None, None, None, None],\n",
    "               ['oceania', 'australia', None, None, None, None],\n",
    "               ['asia', 'croatia', None, None, None, None],\n",
    "               ['asia', 'china', None, None, None, None],\n",
    "               ['asia', 'japan', None, None, None, None],\n",
    "               ['asia', 'malaysia', None, None, None, None],\n",
    "               ['asia', 'philippines', None, None, None, None],\n",
    "               ['asia', 'saudi arabia', None, None, None, None],\n",
    "               ['asia', 'thailand', None, None, None, None],\n",
    "               ['asia', 'qatar', None, None, None, None],\n",
    "               ['asia', 'united arab emirates', None, None, None, None],\n",
    "               ['africa', 'kenya', None, None, None, None],\n",
    "               ['africa', 'south africa', None, None, None, None]]\n",
    "f = open(os.path.join(SARS_DIR, identifier + '.npy'), mode='w+b')\n",
    "np.save(f, selected)\n",
    "f.close()\n",
    "\n",
    "identifier2 = f'regions-{DATA_DATE}b'\n",
    "selected2   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2020-01-01', '2021-01-01'],\n",
    "               ['europe', 'united kingdom', 'northern ireland', None, None, None]]\n",
    "f = open(os.path.join(SARS_DIR, identifier2 + '.npy'), mode='w+b')\n",
    "np.save(f, selected2)\n",
    "f.close()\n",
    "\n",
    "identifier3 = f'regions-{DATA_DATE}c'\n",
    "selected3   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2021-01-01', '2021-03-01']]\n",
    "f = open(os.path.join(SARS_DIR, identifier3 + '.npy'), mode='w+b')\n",
    "np.save(f, selected3)\n",
    "f.close()\n",
    "\n",
    "identifier4 = f'regions-{DATA_DATE}d'\n",
    "selected4   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2021-03-01', '2021-05-01']]\n",
    "f = open(os.path.join(SARS_DIR, identifier4 + '.npy'), mode='w+b')\n",
    "np.save(f, selected4)\n",
    "f.close()\n",
    "\n",
    "identifier5 = f'regions-{DATA_DATE}e'\n",
    "selected5   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2021-05-01', '2021-07-01']]\n",
    "f = open(os.path.join(SARS_DIR, identifier5 + '.npy'), mode='w+b')\n",
    "np.save(f, selected5)\n",
    "f.close()\n",
    "\n",
    "identifier6 = f'regions-{DATA_DATE}f'\n",
    "selected6   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2021-07-01', '2021-09-01']]\n",
    "f = open(os.path.join(SARS_DIR, identifier6 + '.npy'), mode='w+b')\n",
    "np.save(f, selected6)\n",
    "f.close()\n",
    "\n",
    "identifier7 = f'regions-{DATA_DATE}g'\n",
    "selected7   = [['europe', 'united kingdom', ['england', 'wales', 'scotland'], None, '2021-09-01', '2021-11-01']]\n",
    "f = open(os.path.join(SARS_DIR, identifier7 + '.npy'), mode='w+b')\n",
    "np.save(f, selected7)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/processing-multiallele.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/regions-2021-08-14.npy blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/regions-2021-08-14b.npy blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-processing.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-processing.sh\n",
      "\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14 /Users/brianlee/SARS-CoV-2-Data &&\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:EPI_ISL_402125.fasta /Users/brianlee/SARS-CoV-2-Data\n"
     ]
    }
   ],
   "source": [
    "# Transfer scripts, run processing, and extract files.\n",
    "# Processing extracts region specific sequences and eliminates non-polymorphic sites to reduce required computational load of downstream analysis.\n",
    "# Data is also filtered to eliminate sequences with many gaps and sites that are primarily gaps, to impute ambiguous nucleotides.\n",
    "# Sequences are then time-ordered and formatted so that they can be read into the Inference scripts.\n",
    "data_module   = 'data_processing.py'\n",
    "data_mod_path = os.path.join(SCRIPT_DIR, data_module)\n",
    "out_folder    = os.path.join(SSH_DATA, 'data')\n",
    "input_file    = 'merged-final.csv'\n",
    "script_file   = 'processing-multiallele.py'\n",
    "\n",
    "job_file      = 'job-processing.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=900G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/genome-error\n",
    "#SBATCH -o ./MPL/out/genome-out\n",
    "#SBATCH -p highmem\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier   + '.npy'} --find_syn_off --no_trim &&\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier2  + '.npy'} --find_syn_off --no_trim && \n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier3  + '.npy'} --find_syn_off --no_trim &&\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier4  + '.npy'} --find_syn_off --no_trim &&\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier5  + '.npy'} --find_syn_off --no_trim &&\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier6  + '.npy'} --find_syn_off --no_trim &&\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier7  + '.npy'} --find_syn_off --no_trim &&\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "print('scp %s/%s %s. &&' % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SCRIPT_DIR, data_module, SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier2 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier3 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier4 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier5 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier6 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier7 + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s.'    % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "\n",
    "print('scp -r %s%s %s &&' % (SSH_HOME, out_folder, SARS_DIR))\n",
    "print('scp -r %s%s %s' % (SSH_HOME, REF_TAG + '.fasta', SARS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding time series with good sampling in each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/trim-sampling-intervals.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-trim-intervals.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-trim-intervals.sh\n",
      "\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14 /Users/brianlee/SARS-CoV-2-Data/2021-08-14\n"
     ]
    }
   ],
   "source": [
    "# Finding time series with good sampling\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data')\n",
    "output_dir    = input_dir\n",
    "script_file   = 'trim-sampling-intervals.py'\n",
    "\n",
    "job_file      = 'job-trim-intervals.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=300G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/sampling-error\n",
    "#SBATCH -o ./MPL/out/sampling-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_dir {input_dir} -o {output_dir}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "\n",
    "print('scp -r %s%s %s' % (SSH_HOME, output_dir, DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating sites that never rise to a very high frequency or have a mutation only in a few sequences in a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/genome-trimmed | wc -l\n",
      "\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/find-max-frequencies.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-max-freqs.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-max-freqs.sh\n"
     ]
    }
   ],
   "source": [
    "# Finding maximum frequencies in each region\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'genome-trimmed')\n",
    "output_dir    = os.path.join(SSH_DATA, 'data', 'max-freqs')\n",
    "script_file   = 'find-max-frequencies.py'\n",
    "\n",
    "# run on cluster to find number of files\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 161    # the number of data files \n",
    "\n",
    "job_file      = 'job-max-freqs.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freqs-error-%a\n",
    "#SBATCH -o ./MPL/out/freqs-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "python {script_file} --input_file \\\"$file\\\" -o {output_dir}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/combine-max-freqs.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-combine-max-freqs.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-combine-max-freqs.sh\n"
     ]
    }
   ],
   "source": [
    "# Combining maximum frequencies in different regions\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'max-freqs')\n",
    "out_dir       = os.path.join(SSH_DATA, 'data')\n",
    "script_file   = 'combine-max-freqs.py'\n",
    "\n",
    "job_file      = 'job-combine-max-freqs.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freqs-combine-error\n",
    "#SBATCH -o ./MPL/out/freqs-combine-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_dir {input_dir} -o {out_dir}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/filter-sites.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-filter-sites.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-filter-sites.sh\n"
     ]
    }
   ],
   "source": [
    "# Filtering sites\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'genome-trimmed')\n",
    "out_dir       = os.path.join(SSH_DATA, 'data')\n",
    "freq_file     = os.path.join(SSH_DATA, 'data', 'maximum-frequencies.npz')\n",
    "script_file   = 'filter-sites.py'\n",
    "\n",
    "job_file      = 'job-filter-sites.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/filter-error\n",
    "#SBATCH -o ./MPL/out/filter-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_dir {input_dir} -o {out_dir} --max_file {freq_file} --max_freq 0.05 && \\\n",
    "python {script_file} --input_dir {input_dir} -o {out_dir} --max_file {freq_file} --max_freq 0.01 && \\\n",
    "python {script_file} --input_dir {input_dir} -o {out_dir} --max_file {freq_file} --max_freq 0.1\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "print(f'scp -r {SSH_HOME}/{SSH_DATA}/data {SARS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of sequences used in final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sequences used = 1652770.0\n"
     ]
    }
   ],
   "source": [
    "counts = 0\n",
    "for file in os.listdir(os.path.join(DATA_DIR, 'freq_0.05')):\n",
    "    filepath = os.path.join(DATA_DIR, 'freq_0.05', file)\n",
    "    nVec     = np.load(filepath, allow_pickle=True)['nVec']\n",
    "    counts  += np.sum([np.sum(nVec[t]) for t in range(len(nVec))])\n",
    "print(f'total number of sequences used = {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='link'></a>\n",
    "# Finding linked sites and classifying mutations as synonymous or nonsynonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding linked sites on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.05 | wc -l\n",
      "scp /Users/brianlee/Python/MPL/epi-covar-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/Python/MPL/epi-find-linked-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive-alt2 blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-counts.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-linked.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-counts.sh\n",
      "sbatch job-find-linked.sh\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/linked-sites.npy /Users/brianlee/SARS-CoV-2-Data/2021-08-14 &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/linked-sites-alleles.npy /Users/brianlee/SARS-CoV-2-Data/2021-08-14 &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/linked-sites-anywhere.npy /Users/brianlee/SARS-CoV-2-Data/2021-08-14 &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/nucleotide-counts.npz /Users/brianlee/SARS-CoV-2-Data/2021-08-14\n"
     ]
    }
   ],
   "source": [
    "# Finding single and double site counts in each region\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'freq_0.01')\n",
    "count_script  = 'epi-covar-parallel.py'\n",
    "link_script   = 'epi-find-linked-parallel.py'\n",
    "archive_loc   = os.path.join(SCRIPT_DIR, 'Archive-alt-vector')\n",
    "out_dir       = os.path.join(SSH_DATA, 'data')\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'counts')\n",
    "\n",
    "# run on cluster to find number of files\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "num_files = 161    # the number of data files \n",
    "\n",
    "job_file1 = 'job-find-counts.sh'\n",
    "job_str1  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=500G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/counts-error-%a\n",
    "#SBATCH -o ./MPL/out/counts-out-%a\n",
    "#SBATCH -p highmem\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "cd ./Archive-alt-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{count_script} --data \\\"$file\\\" -o $tempout --find_counts --pop_size 10000 -k 0.1 -R 2 --scratch /scratch/counts --timed 1\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "# combining single and double site counts from the different regions to find linked sites\n",
    "\n",
    "job_file2 = 'job-find-linked.sh'\n",
    "job_str2  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=250G\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/linked-error\n",
    "#SBATCH -o ./MPL/out/linked-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {link_script} --data {temp_dir} --out_dir {out_dir} --timed 1 --multisite -q 5 --link_tol 0.8 --findLinkedRegional\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, count_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, link_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file2, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s' % job_file1)\n",
    "print('sbatch %s' % job_file2)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print('scp %s%s/linked-sites.npy %s &&'               % (SSH_HOME, out_dir, DATA_DIR))\n",
    "print('scp %s%s/linked-sites-regional.npy %s &&'      % (SSH_HOME, out_dir, DATA_DIR))\n",
    "print('scp %s%s/nucleotide-counts.npz %s' % (SSH_HOME, out_dir, DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon        = ['NSP13-260-0-T', 'S-13-1-T', 'S-152-2-T', 'NC-28271-T', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', \n",
    "                  'S-452-1-G', 'N-205-1-T', 'ORF3a-57-2-T', 'NSP9-65-0-G'] # Done\n",
    "\n",
    "alpha          = ['NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--',\n",
    "                  'NSP6-108-1--', 'NSP6-108-2--', 'S-501-0-T', 'NSP12-412-2-T', 'NSP2-36-2-T', 'NSP3-183-1-T', 'NSP3-890-1-A', \n",
    "                  'NSP3-1089-2-T', 'NSP3-1412-1-C', 'NSP12-613-2-T', 'NSP12-912-2-C', 'S-68-1--', 'S-68-2--', 'S-69-0--', 'S-69-1--', \n",
    "                  'S-69-2--', 'S-70-0--', 'S-143-2--', 'S-144-0--', 'S-144-1--', 'S-570-1-A', 'S-681-1-A', 'S-716-1-T', 'S-982-0-G', \n",
    "                  'S-1118-0-C', 'ORF8-27-0-T', 'ORF8-52-1-T', 'ORF8-73-1-G', 'N-3-0-C', 'N-3-1-T', 'N-3-2-A', 'N-235-1-T',\n",
    "                  'N-203-1-A', 'N-203-2-A', 'N-204-0-C', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G']    # Done\n",
    "\n",
    "gamma          = ['NSP9-31-2-T', 'NSP1-156-2-C', 'NSP3-10-2-T', 'NSP3-370-1-T', 'NSP3-977-0-C', 'NSP3-1200-2-G', 'NSP3-1298-2-G', \n",
    "                  'NSP12-140-2-T', 'NSP13-341-2-T', 'S-20-1-A', 'S-417-1-C', 'S-1027-1-T', 'ORF3a-253-0-C', 'ORF8-92-0-A', 'N-80-1-G', \n",
    "                  'S-190-2-T', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', 'S-501-0-T', 'NSP6-106-0--', 'NSP6-106-1--', \n",
    "                  'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--', 'NSP6-108-1--', 'NSP6-108-2--',\n",
    "                  'N-203-1-A', 'N-203-2-A', 'N-204-0-C', 'N-202-0-T', 'N-202-1-C', 'S-18-0-T', 'S-26-0-T', 'S-655-0-T',\n",
    "                  'S-1176-0-T']    # Done\n",
    "\n",
    "twentyE_EU1    = ['NSP16-199-2-C', 'NSP1-60-2-C', 'NSP3-1189-2-T', 'M-93-2-G', 'N-220-1-T', 'ORF10-30-0-T', 'S-222-1-T',\n",
    "                  'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G']    # Done\n",
    "# This is Pango lineage B.1.177\n",
    "\n",
    "delta          = ['NSP12-671-0-A', 'NC-209-T', 'NSP13-77-1-T', 'S-19-1-G', 'S-156-1--', 'S-156-2--', 'S-157-0--', 'S-157-1--', 'S-157-2--', \n",
    "                  'S-158-0--', 'S-478-1-A', 'S-681-1-G', 'S-950-0-A', 'ORF3a-26-1-T', 'M-82-1-C', 'ORF7a-82-1-C', 'ORF7a-120-1-T', \n",
    "                  'ORF8-119-0--', 'ORF8-119-1--', 'ORF8-119-2--', 'ORF8-120-0--', 'ORF8-120-1--', 'ORF8-120-2--', 'N-63-1-G', 'N-203-1-T', \n",
    "                  'N-377-0-T', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', 'S-452-1-G', 'NC-28270--', 'NSP4-492-1-T']    # Done\n",
    "\n",
    "beta           = ['S-80-1-C', 'NSP3-837-2-T', 'S-215-1-G', 'E-71-1-T', 'S-240-2--', 'S-241-0--', 'S-242-1--', 'S-242-2--', 'S-243-0--', \n",
    "                  'S-240-1--', 'S-241-1--', 'S-241-2--', 'S-242-0--', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', 'S-501-0-T',\n",
    "                  'NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--', \n",
    "                  'NSP6-108-1--', 'NSP6-108-2--', 'S-417-2-T', 'S-484-0-A', 'S-701-1-T', 'ORF3a-57-2-T', 'NSP2-85-1-T', 'NSP5-90-1-G',\n",
    "                  'N-205-1-T', 'NC-173-T', 'ORF8-120-2-T']    # Done\n",
    "\n",
    "lambda_new     = ['S-246-2--', 'NSP3-1569-0-G', 'S-247-0--', 'S-247-1--', 'S-247-2--', 'S-248-0--', 'S-248-1--', 'S-248-2--', 'S-249-0--', \n",
    "                  'S-249-1--', 'S-249-2--', 'S-250-0--', 'S-250-1--', 'S-250-2--', 'S-251-0--', 'S-251-1--', 'S-251-2--', 'S-252-0--', \n",
    "                  'S-252-1--', 'S-252-2--', 'S-253-0--', 'N-214-0-T', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', \n",
    "                  'NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--', \n",
    "                  'NSP6-108-1--', 'NSP6-108-2--', 'S-75-1-T', 'S-76-1-T', 'S-452-1-A', 'S-490-1-C', 'S-859-1-A', 'NSP3-428-1-T',\n",
    "                  'NSP3-1469-0-T', 'NSP4-438-1-C', 'NSP4-492-1-T', 'NSP5-15-0-A'] # Done\n",
    "\n",
    "iota           = ['N-234-2-A', 'S-5-0-T', 'S-95-1-T', 'S-253-1-G', 'S-484-0-A', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G',\n",
    "                 'S-701-1-T', 'NSP13-88-2-C', 'ORF3a-42-1-T', 'ORF3a-67-2-T', 'NSP2-85-1-T', 'NSP4-438-1-C', 'NSP6-106-0--', 'NSP6-106-1--', \n",
    "                 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--',  'NSP6-108-1--', 'NSP6-108-2--', 'N-199-1-T',\n",
    "                 'N-232-2-A', 'ORF8-11-1-T', 'NSP15-214-2-G', 'NC-28270--']\n",
    "\n",
    "D614G          = ['NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G']    # Done\n",
    "\n",
    "B1_1_318       = ['NSP15-320-0-A', 'S-575-2-C', 'S-1238-2-A', 'ORF7b-44-1--', 'NC-27887--', 'NC-27888--', 'NC-27889--', 'NC-27890--', \n",
    "                  'NC-27891--', 'NC-27892--', 'ORF8-1-0--', 'ORF8-1-1--', 'ORF8-1-2--', 'NSP4-173-1-T', 'S-796-0-C', 'ORF8-2-0--', \n",
    "                  'ORF8-2-1--', 'ORF8-2-2--', 'ORF8-3-0--', 'ORF8-3-1--', 'NC-28270-G', 'N-208-2--', 'N-209-0--', 'N-208-1--',\n",
    "                  'NSP3-378-1-T', 'NSP3-1693-2-T', 'NSP5-21-1-T', 'NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', \n",
    "                  'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--', 'NSP6-108-1--', 'NSP6-108-2--', 'S-95-1-T', 'S-143-2--', 'S-144-0--', \n",
    "                  'S-144-1--', 'S-484-0-A', 'NC-240-T', 'NSP3-106-2-T', 'NSP12-323-1-T', 'S-614-1-G', 'S-681-1-A', 'M-82-1-C',\n",
    "                  'N-203-1-A', 'N-203-2-A', 'N-204-0-C'] # Add N-234-2-T, Many others\n",
    "\n",
    "#all_variants   = [epsilon, alpha, beta, gamma, lambda_new, delta, twentyE_EU1, iota, D614G]\n",
    "all_variants   = [epsilon, alpha, beta, gamma, lambda_new, delta, twentyE_EU1, D614G]\n",
    "#all_variants    = [epsilon, alpha, beta, gamma, lambda_new, delta, twentyE_EU1, iota, D614G, B1_1_318]\n",
    "np.save(os.path.join(SARS_DIR, 'variants.npy'), all_variants)\n",
    "\n",
    "var_dic = {'alpha' : alpha, 'epsilon' : epsilon, 'beta' : beta, 'gamma' : gamma, 'lambda' : lambda_new, 'delta' : delta, '20E\\(EU1\\)' : twentyE_EU1, 'B.1' : D614G}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying mutations as synonymous or nonsynonymous on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/find-nonsynonymous.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-nonsynonymous.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-nonsynonymous.sh\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/synonymous.npz /Users/brianlee/SARS-CoV-2-Data/2021-08-14 &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/synonymous-prot.npz /Users/brianlee/SARS-CoV-2-Data/2021-08-14 &&\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/protein-changes /Users/brianlee/SARS-CoV-2-Data/2021-08-14\n"
     ]
    }
   ],
   "source": [
    "script_file   = 'find-nonsynonymous.py'\n",
    "out_dir       = os.path.join(SSH_DATA, 'data')\n",
    "\n",
    "job_file      = 'job-find-nonsynonymous.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=300G\n",
    "#SBATCH --time=1-0\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/error-synonymous\n",
    "#SBATCH -o ./MPL/out/out-synonymous\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --data {out_dir} --multisite\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "print('scp %s/%s %s. &&'    % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'    % (SCRIPT_DIR, 'data_processing.py', SSH_HOME))\n",
    "#print('scp %s/%s %s/%s &&'  % (SARS_DIR, 'EPI_ISL_402125.fasta', SSH_HOME, SSH_DATA))\n",
    "print('scp %s/%s %s.'       % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "\n",
    "print('scp %s%s/%s %s &&' % (SSH_HOME, out_dir, 'synonymous.npz', DATA_DIR))\n",
    "print('scp %s%s/%s %s &&' % (SSH_HOME, out_dir, 'synonymous-prot.npz', DATA_DIR))\n",
    "print('scp %s%s/%s %s'    % (SSH_HOME, out_dir, 'protein-changes', DATA_DIR))\n",
    "print(f'scp {SSH_HOME}/{SSH_DATA}/{REF_TAG}.fasta {SARS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the amino acid for nucleotide mutations that are linked to other nucleotide mutations in the same codon\n",
    "\n",
    "# RENAME protein-changes-test and synonymous-test IN SUBSEQUENT CELLS AND THEN DELETE THE CREATION OF THESE FILES\n",
    "\n",
    "dp.fix_aa_changes(os.path.join(DATA_DIR, 'synonymous-prot.npz'),\n",
    "                  os.path.join(DATA_DIR, 'linked-sites.npy'),\n",
    "                  os.path.join(DATA_DIR, 'synonymous-prot-test'))\n",
    "\n",
    "data = np.load(os.path.join(DATA_DIR, 'synonymous-prot-test.npz'), allow_pickle=True)\n",
    "types = data['types']\n",
    "nucs = data['nuc_index']\n",
    "locs = data['locations']\n",
    "aa_changes = data['aa_changes']\n",
    "\n",
    "prot_out_file = os.path.join(DATA_DIR, 'protein-changes-test')\n",
    "g = open(prot_out_file, 'w')\n",
    "for i in range(len(locs)):\n",
    "    g.write('%s,%s\\n' % (locs[i], aa_changes[i]))\n",
    "g.close()\n",
    "\n",
    "prot_out_file = os.path.join(DATA_DIR, 'protein-changes')\n",
    "g = open(prot_out_file, 'w')\n",
    "for i in range(len(locs)):\n",
    "    g.write('%s,%s\\n' % (locs[i], aa_changes[i]))\n",
    "g.close()\n",
    "    \n",
    "syn_out_file = os.path.join(DATA_DIR, 'synonymous-test.npz')\n",
    "np.savez_compressed(syn_out_file, locations=locs, types=types)\n",
    "syn_out_file = os.path.join(DATA_DIR, 'synonymous.npz')\n",
    "np.savez_compressed(syn_out_file, locations=locs, types=types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='infer'></a>\n",
    "# Inferring selection coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the selection coefficients for all of the sites using all of the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.05 | wc -l\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-covar-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-freqs-multisite.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-multisite.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-freqs-multisite.sh &&\n",
      "sbatch job-infer-multisite.sh &&\n",
      "mkdir /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14 &&\n",
      "mv /rhome/blee098/bigdata/SARS-CoV-2-Data/infer-2021-08-14-* /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14\n",
      "\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14 /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "# infering the selection coefficients using a cutoff frequency of 5%\n",
    "input_dir      = os.path.join(SSH_DATA, 'data', 'freq_0.05')\n",
    "freq_script    = 'epi-covar-parallel.py'\n",
    "inf_script     = 'epi-inf-parallel.py'\n",
    "archive_loc    = os.path.join(SCRIPT_DIR, 'Archive-vector')\n",
    "temp_dir       = os.path.join(SSH_DATA, 'data', 'freqs')\n",
    "out_file       = os.path.join(SSH_DATA, f'infer-{DATA_DATE}')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 161    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-freqs-multisite.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=250G\n",
    "#SBATCH --time=4-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq-error-%a\n",
    "#SBATCH -o ./MPL/out/freq-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "cd ./Archive-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch /scratch/freqs --timed 1\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "regularization=[1, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 100, 125, 150, 175, 200]\n",
    "\n",
    "job_file2 = 'job-infer-multisite.sh'\n",
    "job_str2  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf-error-%a\n",
    "#SBATCH -o ./MPL/out/inf-out-%a\n",
    "#SBATCH -p highmem\n",
    "#SBATCH --array=0-16\n",
    "\n",
    "regularization=(1 5 10 15 20 25 30 40 50 60 70 80 100 125 150 175 200)\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "outfile={out_file}-g-${{regularization[$SLURM_ARRAY_TASK_ID]}}\n",
    "python {inf_script} --data {temp_dir} --timed 1 -o $outfile -q 5 --g1 ${{regularization[$SLURM_ARRAY_TASK_ID]}}\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file2, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file1)\n",
    "print('sbatch %s &&' % job_file2)\n",
    "print('mkdir %s &&'  % os.path.join(SSH_DATA, f'inf-{DATA_DATE}'))\n",
    "print('mv %s %s'     % (os.path.join(SSH_DATA, f'infer-{DATA_DATE}-*'), os.path.join(SSH_DATA, f'inf-{DATA_DATE}')))\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print('scp -r %s%s/%s %s' % (SSH_HOME, SSH_DATA, f'inf-{DATA_DATE}', INF_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating mutations that are not observed and reference nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "selection data filtered\n"
     ]
    }
   ],
   "source": [
    "cutoff    = 0\n",
    "regs      = [20, 30, 40, 50, 60]\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'sensitivity-data')):\n",
    "    os.mkdir(os.path.join(DATA_DIR, 'sensitivity-data'))\n",
    "for reg in regs:\n",
    "\n",
    "    inf_data  = np.load(os.path.join(INF_DIR, f'inf-{DATA_DATE}', f'infer-{DATA_DATE}-g-{reg}.npz'), allow_pickle=True)\n",
    "    alleles   = inf_data['allele_number']\n",
    "    selection = inf_data['selection']\n",
    "    errors    = inf_data['error_bars']\n",
    "    s_ind     = inf_data['selection_independent']\n",
    "    numerator = inf_data['numerator']\n",
    "    covar_int = inf_data['covar_int']\n",
    "    locations = inf_data['locations']\n",
    "    times     = inf_data['times']\n",
    "    \n",
    "    data      = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "    muts      = data['allele_number']\n",
    "    counts    = data['counts']\n",
    "    mask      = np.isin(muts, alleles)\n",
    "    counts    = counts[mask]\n",
    "\n",
    "    #print('data loaded')\n",
    "    clip_start= 150\n",
    "    nucs      = np.array([i[:-2] for i in alleles], dtype=int)\n",
    "\n",
    "    mask      = np.nonzero(np.logical_and(np.logical_and(counts>cutoff, selection!=0), nucs > clip_start))[0]\n",
    "\n",
    "    selection = selection[mask]\n",
    "    alleles   = alleles[mask]\n",
    "    errors    = errors[mask]\n",
    "    s_ind     = s_ind[mask]\n",
    "    numerator = numerator[mask]\n",
    "    covar_int = covar_int[mask][:, mask]\n",
    "\n",
    "    #print('selection data filtered')\n",
    "    \n",
    "    np.savez_compressed(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-observed.npz'), covar_int=covar_int, \n",
    "                       selection=selection, selection_independent=s_ind, error_bars=errors, numerator=numerator,\n",
    "                       allele_number=alleles)\n",
    "    \n",
    "    dp.website_file2(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-observed.npz'),\n",
    "                     os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                     os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                     os.path.join(DATA_DIR, f'selection-g-40-observed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate noncoding sites and rerun inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate all noncoding sites\n",
    "\n",
    "data      = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "muts      = data['allele_number']\n",
    "counts    = data['counts']\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'inf-{DATA_DATE}', f'infer-{DATA_DATE}-g-40.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "selection = inf_data['selection']\n",
    "errors    = inf_data['error_bars']\n",
    "s_ind     = inf_data['selection_independent']\n",
    "numerator = inf_data['numerator']\n",
    "covar_int = inf_data['covar_int']\n",
    "locations = inf_data['locations']\n",
    "proteins  = [dp.get_label2(i).split('-')[0] for i in alleles]\n",
    "\n",
    "mask      = np.isin(muts, alleles)\n",
    "muts      = muts[mask]\n",
    "counts    = counts[mask]\n",
    "\n",
    "mask      = np.nonzero(np.array(proteins)!='NC')[0]\n",
    "selection = selection[mask]\n",
    "alleles   = alleles[mask]\n",
    "errors    = errors[mask]\n",
    "s_ind     = s_ind[mask]\n",
    "numerator = numerator[mask]\n",
    "covar_int = covar_int[mask][:, mask]\n",
    "counts    = counts[mask]\n",
    "\n",
    "q   = 5\n",
    "N   = 10000\n",
    "k   = 0.1\n",
    "g   = 40\n",
    "R   = 2\n",
    "g1  = np.array(g, dtype=float) * (N * k * R) / (R + k)\n",
    "for i in range(len(covar_int)):\n",
    "    covar_int[i, i] += g1\n",
    "\n",
    "# infer selection coefficients\n",
    "s      = linalg.solve(covar_int, numerator, assume_a='sym')\n",
    "s_ind  = numerator / np.diag(covar_int)\n",
    "errors = 1 / np.sqrt(np.absolute(np.diag(covar_int)))\n",
    "\n",
    "# normalize reference nucleotide to zero\n",
    "L = int(len(s) / 5)\n",
    "selection  = np.reshape(s, (L, q))\n",
    "selection_nocovar = np.reshape(s_ind, (L, q))\n",
    "ref_seq, ref_tag  = dp.get_MSA(REF_TAG +'.fasta')\n",
    "ref_seq  = list(ref_seq[0])\n",
    "allele_number = np.unique([int(i[:-2]) for i in alleles])\n",
    "ref_poly = np.array(ref_seq)[allele_number]\n",
    "\n",
    "s_new = []\n",
    "s_SL  = []\n",
    "for i in range(L):\n",
    "    idx = NUC.index(ref_poly[i])\n",
    "    temp_s    = selection[i]\n",
    "    temp_s    = temp_s - temp_s[idx]\n",
    "    temp_s_SL = selection_nocovar[i]\n",
    "    temp_s_SL = temp_s_SL - temp_s_SL[idx]\n",
    "    s_new.append(temp_s)\n",
    "    s_SL.append(temp_s_SL)\n",
    "selection         = s_new\n",
    "selection_nocovar = s_SL\n",
    "\n",
    "selection         = np.array(selection).flatten()\n",
    "selection_nocovar = np.array(selection_nocovar).flatten()\n",
    "error_bars        = errors\n",
    "\n",
    "\n",
    "mask      = np.nonzero(np.logical_and(counts>0, selection!=0))[0]\n",
    "selection = selection[mask]\n",
    "alleles   = alleles[mask]\n",
    "errors    = errors[mask]\n",
    "s_ind     = s_ind[mask]\n",
    "numerator = numerator[mask]\n",
    "covar_int = covar_int[mask][:, mask]\n",
    "\n",
    "print('selection data filtered')\n",
    "    \n",
    "np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-40-observed-coding.npz'), covar_int=covar_int, \n",
    "                   selection=selection, selection_independent=s_ind, error_bars=errors, numerator=numerator,\n",
    "                   allele_number=alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23164\n",
      "23164\n"
     ]
    }
   ],
   "source": [
    "dp.website_file2(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-40-observed-coding.npz'),\n",
    "                 os.path.join(DATA_DIR, 'synonymous-prot.npz'),\n",
    "                 os.path.join(DATA_DIR, 'linked-sites.npy'),\n",
    "                 os.path.join(DATA_DIR, f'selection-g-40-observed-coding.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infering the selection coefficients using a cutoff frequency of 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.01 | wc -l\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-covar-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-freqs2.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-multisite2.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-freqs2.sh &&\n",
      "sbatch job-infer-multisite2.sh\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/infer-2021-08-14-cutoff0.01.npz /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "# infering the selection coefficients using a cutoff frequency of 1%\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'freq_0.01')\n",
    "freq_script   = 'epi-covar-parallel.py'\n",
    "inf_script    = 'epi-inf-parallel.py'\n",
    "archive_loc   = os.path.join(SCRIPT_DIR, 'Archive-vector')\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'freqs-0.01')\n",
    "out_file      = os.path.join(SSH_DATA, f'infer-{DATA_DATE}-1pct')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 161    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file3     = 'job-freqs2.sh'\n",
    "job_str3      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --time=3-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq2-error-%a\n",
    "#SBATCH -o ./MPL/out/freq2-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "cd ./Archive-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file3), 'w')\n",
    "f.write('%s\\n' % job_str3)\n",
    "f.close()\n",
    "\n",
    "job_file4 = 'job-infer2.sh'\n",
    "job_str4  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=250G\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf2-error\n",
    "#SBATCH -o ./MPL/out/inf2-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {inf_script} --data {temp_dir} --timed 1 -o {out_file} -q 5 --g1 40 --eliminateNC\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file4), 'w')\n",
    "f.write('%s\\n' % job_str4)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file3, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file4, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file3)\n",
    "print('sbatch %s' % job_file4)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print('scp %s%s %s'      % (SSH_HOME, out_file + '.npz', INF_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.01 | wc -l\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-traj-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-traj.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "\n",
      "sbatch job-find-traj.sh &&\n",
      "\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/trajectories-1pct /Users/brianlee/SARS-CoV-2-Data/2021-08-14\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency trajectories\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'freq_0.01')\n",
    "freq_script   = 'epi-traj-parallel.py'\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'trajectories-1pct')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 161    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-traj.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=150G\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq-uk-error-%a\n",
    "#SBATCH -o ./MPL/out/freq-uk-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python {freq_script} --data \\\"$file\\\" -o $tempout -q 5 --timed 1 --window 10\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file1)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print(f'scp -r {SSH_HOME}{temp_dir} {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine trajectory files\n",
    "traj      = []\n",
    "muts      = []\n",
    "times     = []\n",
    "locs      = []\n",
    "t_full    = []\n",
    "traj_full = []\n",
    "traj_dir  = os.path.join(DATA_DIR, 'trajectories-1pct')\n",
    "for file in os.listdir(traj_dir):\n",
    "    data = np.load(os.path.join(traj_dir, file))\n",
    "    times.append(data['times'])\n",
    "    muts.append(data['allele_number'])\n",
    "    traj.append(data['traj'])\n",
    "    locs.append(file[:-4])\n",
    "\n",
    "np.savez_compressed(os.path.join(DATA_DIR, 'trajectories-1pct.npz'),\n",
    "                    times=times, mutant_sites=muts, traj=traj, locations=locs, \n",
    "                    times_nosmooth=t_full, traj_nosmooth=traj_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trajectories for 20E-EU1\n",
    "os.chdir(SCRIPT_DIR)\n",
    "import data_processing as dp\n",
    "reload(dp)\n",
    "os.chdir(SARS_DIR)\n",
    "times    = []\n",
    "traj     = []\n",
    "data_dir = os.path.join(DATA_DIR, 'freq_0.05')\n",
    "for file in os.listdir(data_dir):\n",
    "    loc_split = file.split('-')\n",
    "    if loc_split[0]=='europe' and loc_split[1]=='united kingdom' and loc_split[2]=='england_wales_scotland':\n",
    "        filepath = os.path.join(data_dir, file)\n",
    "        data     = np.load(filepath, allow_pickle=True)\n",
    "        nVec     = data['nVec']\n",
    "        sVec     = data['sVec']\n",
    "        t_temp   = data['times']\n",
    "        muts     = data['mutant_sites']\n",
    "        muts     = [dp.get_label(i) for i in muts]\n",
    "        traj_temp= dp.trajectory_calc_20e_eu1(nVec, sVec, muts, d=5)\n",
    "        traj_temp= dp.moving_average(traj_temp, window=10)\n",
    "        traj.append(traj_temp)\n",
    "        times.append(t_temp[9:])\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, '20E-EU1-trajectory.csv'), mode='w')\n",
    "f.write('time,frequency\\n')\n",
    "for i in range(len(times)):\n",
    "    for j in range(len(times[i])):\n",
    "        f.write(f'{times[i][j]},{traj[i][j]:0.8f}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mutations in is 79696.0\n"
     ]
    }
   ],
   "source": [
    "# Finding number of sites in final inference\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-1pct.npz'), allow_pickle=True)\n",
    "muts      = inf_data['allele_number']\n",
    "print(f'number of mutations in is {4 * len(muts) / 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating sites that are infrequently observed (frequency never about 1% in any region and total observations > 5)\n",
    "reg       = 40\n",
    "count_min = 5\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-1pct.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "selection = inf_data['selection']\n",
    "errors    = inf_data['error_bars']\n",
    "s_ind     = inf_data['selection_independent']\n",
    "locations = inf_data['locations']\n",
    "times     = inf_data['times']\n",
    "clip_start= 150\n",
    "nucs      = np.array([i[:-2] for i in alleles], dtype=int)\n",
    "\n",
    "# Eliminating reference nucleotides and mutations that never rise above a frequency of 1%\n",
    "traj_dir       = os.path.join(DATA_DIR, 'trajectories-1pct')\n",
    "max_freq       = np.zeros(len(alleles))\n",
    "alleles_sorted = np.argsort(alleles)\n",
    "\n",
    "for file in os.listdir(traj_dir):\n",
    "    data = np.load(os.path.join(traj_dir, file), allow_pickle=True)\n",
    "    muts = data['allele_number']\n",
    "    traj = data['traj']\n",
    "    mask = np.isin(muts, alleles)\n",
    "    muts = muts[mask]\n",
    "    traj = traj[:, mask]\n",
    "    maxs = np.amax(traj, axis=0)\n",
    "    pos  = np.searchsorted(alleles[alleles_sorted], muts)\n",
    "    pos  = alleles_sorted[pos]\n",
    "    for j in range(len(maxs)):\n",
    "        max_freq[pos[j]] = max(max_freq[pos[j]], maxs[j])\n",
    "        \n",
    "        \n",
    "cutoff    = 0.01\n",
    "mask      = np.nonzero(np.logical_and(max_freq>cutoff, selection!=0))[0]\n",
    "\n",
    "selection = selection[mask]\n",
    "alleles   = alleles[mask]\n",
    "errors    = errors[mask]\n",
    "s_ind     = s_ind[mask]\n",
    "\n",
    "count_data = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "counts     = count_data['counts']\n",
    "alleles2   = count_data['allele_number']\n",
    "\n",
    "mask1      = np.isin(alleles2, alleles)\n",
    "alleles2   = alleles2[mask1]\n",
    "counts     = counts[mask1]\n",
    "\n",
    "mask2      = np.isin(alleles, alleles2)\n",
    "alleles    = alleles[mask2]\n",
    "selection  = selection[mask2]\n",
    "s_ind      = s_ind[mask2]\n",
    "errors     = errors[mask2]\n",
    "\n",
    "mask3      = np.nonzero(counts>count_min)[0]\n",
    "alleles    = alleles[mask3]\n",
    "selection  = selection[mask3]\n",
    "s_ind      = s_ind[mask3]\n",
    "errors     = errors[mask3]\n",
    "\n",
    "traj_data = np.load(os.path.join(DATA_DIR, 'trajectories-1pct.npz'), allow_pickle=True)\n",
    "times     = traj_data['times']\n",
    "muts      = traj_data['mutant_sites']\n",
    "locations = traj_data['locations']\n",
    "traj      = traj_data['traj']\n",
    "    \n",
    "np.savez_compressed(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-1pct-observed.npz'),\n",
    "                    selection=selection, selection_independent=s_ind, error_bars=errors,\n",
    "                    allele_number=alleles, traj=traj, mutant_sites=muts, locations=locations,\n",
    "                    times=times)\n",
    "\n",
    "#np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-{reg}-1pct-observed.npz'),\n",
    "#                    selection=selection, selection_independent=s_ind, error_bars=errors,\n",
    "#                    allele_number=alleles, traj=traj, mutant_sites=muts, locations=locations,\n",
    "#                    times=times)\n",
    "    \n",
    "dp.website_file2(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-1pct-observed.npz'),\n",
    "                 os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                 os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                 os.path.join(DATA_DIR, f'selection-g-40-1pct-observed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observed mutations is 21050\n"
     ]
    }
   ],
   "source": [
    "# Number of observed amino acid mutations\n",
    "data = np.load(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-40-1pct-observed.npz'), allow_pickle=True)\n",
    "muts = data['allele_number']\n",
    "print(f'number of observed mutations is {len(muts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding coefficients for variants__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.linked_analysis_alt(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-40-1pct-observed.npz'),\n",
    "                       os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                       os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                       os.path.join(SARS_DIR, 'variants.npy'),\n",
    "                       os.path.join(DATA_DIR, 'linked-coefficients-g-40-1pct.csv'))\n",
    "\n",
    "# Make a csv file containing the frequency trajectories for the different variants in different regions\n",
    "dp.linked_traj_csv(os.path.join(DATA_DIR, 'linked-coefficients-g-40-1pct.csv'), \n",
    "                   os.path.join(DATA_DIR, 'trajectories-1pct.npz'), \n",
    "                   os.path.join(DATA_DIR, 'linked-trajectories-1pct'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change nucleotide mutations so they are indexed at 1 instead of 0\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'linked-coefficients-g-40-1pct.csv'), index_col=False)\n",
    "nucs = list(df['nucleotides'])\n",
    "nucs = [i.split('/') for i in nucs]\n",
    "nucs_new = []\n",
    "for i in nucs:\n",
    "    nucs_temp = []\n",
    "    for j in i:\n",
    "        if j!='not_present':\n",
    "            new_nuc = str(int(j) + 1)\n",
    "        else:\n",
    "            new_nuc = j\n",
    "        nucs_temp.append(new_nuc)\n",
    "    nucs_new.append(nucs_temp)\n",
    "nucs = ['/'.join(i) for i in nucs_new]\n",
    "df['nucleotides'] = nucs\n",
    "df.to_csv(os.path.join(DATA_DIR, 'linked-coefficients-g-40-1pct.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsynonymous inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating synonymous mutations\n",
    "cutoff     = 0\n",
    "\n",
    "count_data = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "counts     = count_data['counts']\n",
    "alleles2   = count_data['allele_number']\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-1pct.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "numerator = inf_data['numerator']\n",
    "covar_int = inf_data['covar_int']\n",
    "\n",
    "mask2     = np.isin(alleles, alleles2)\n",
    "alleles   = alleles[mask2]\n",
    "numerator = numerator[mask2]\n",
    "covar_int = covar_int[mask2][:, mask2]\n",
    "\n",
    "syn_data  = np.load(os.path.join(DATA_DIR, 'synonymous-prot-test.npz'))\n",
    "syn_muts  = syn_data['nuc_index']\n",
    "types     = syn_data['types']\n",
    "types     = types[np.isin(syn_muts, alleles)]\n",
    "\n",
    "idxs_keep = []\n",
    "for i in range(int(len(alleles) / 5)):\n",
    "    types_temp  = types[5 * i : 5 * (i + 1)]\n",
    "    counts_temp = counts[5 * i : 5 * (i + 1)]\n",
    "    types_temp  = [types_temp[j] for j in range(len(types_temp)) if counts_temp[j] > cutoff]\n",
    "    if 'NS' in types_temp:\n",
    "        for j in range(5):\n",
    "            idxs_keep.append((5 * i) + j)\n",
    "idxs_keep = np.array(idxs_keep)\n",
    "\n",
    "numerator = numerator[idxs_keep]\n",
    "covar_int = covar_int[idxs_keep][:, idxs_keep]\n",
    "alleles   = alleles[idxs_keep]\n",
    "counts    = counts[idxs_keep]\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR, f'inf-data-nonsynonymous-{DATA_DATE}-1pct.npz'), allele_number=alleles, \n",
    "                    numerator=numerator, covar_int=covar_int, counts=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferring the selection coefficients only for nonsynonymous sites \n",
    "inf_script    = 'epi-inf-nonsynonymous.py'\n",
    "count_file    = 'nucleotide-counts.npz'\n",
    "input_file    = f'infer-{DATA_DATE}-1pct.npz'\n",
    "syn_file      = 'synonymous-prot-test.npz'\n",
    "out_file      = os.path.join(SSH_DATA, f'infer-{DATA_DATE}-1pct-nonsyn-observed-v2')\n",
    "\n",
    "job_file2 = 'job-infer-nonsyn.sh'\n",
    "job_str2  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH -e ./MPL/out/inf-1pct-error\n",
    "#SBATCH -o ./MPL/out/inf-1pct-out\n",
    "#SBATCH -p batch\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "python {inf_script} --data {SSH_DATA}/{input_file} -k 0.1 -R 2 --pop_size 10000 --timed 1 --g1 40  -o {out_file} --syn_data {syn_file} --count_data {SSH_DATA}/{count_file}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print(f'scp {INF_DIR}/{input_file} {SSH_HOME}{SSH_DATA} &&')\n",
    "print(f'scp {DATA_DIR}/{count_file} {SSH_HOME}{SSH_DATA} &&')\n",
    "print(f'scp {DATA_DIR}/{syn_file} {SSH_HOME} &&')\n",
    "print(f'scp {os.path.join(SCRIPT_DIR, job_file2)} {SSH_HOME} ')\n",
    "print('')\n",
    "print(f'scp {SSH_HOME}{out_file}-unmasked.npz {INF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking out mutations never above 1% frequency\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-1pct-nonsyn-observed-v2-unmasked.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "selection = inf_data['selection']\n",
    "errors    = inf_data['error_bars']\n",
    "s_ind     = inf_data['selection_independent']\n",
    "mut_sites = [dp.get_label2(i) for i in alleles]\n",
    "\n",
    "syn_data  = np.load(os.path.join(DATA_DIR, 'synonymous-prot-test.npz'))\n",
    "syn_muts  = syn_data['nuc_index']\n",
    "types     = syn_data['types']\n",
    "types     = types[np.isin(syn_muts, alleles)]\n",
    "\n",
    "# Finding maximum frequencies\n",
    "traj_dir       = os.path.join(DATA_DIR, 'trajectories-1pct')\n",
    "max_freq       = np.zeros(len(alleles))\n",
    "alleles_sorted = np.argsort(alleles)\n",
    "for file in os.listdir(traj_dir):\n",
    "    data = np.load(os.path.join(traj_dir, file), allow_pickle=True)\n",
    "    muts = data['allele_number']\n",
    "    traj = data['traj']\n",
    "    mask = np.isin(muts, alleles)\n",
    "    muts = muts[mask]\n",
    "    traj = traj[:, mask]\n",
    "    maxs = np.amax(traj, axis=0)\n",
    "    pos  = np.searchsorted(alleles[alleles_sorted], muts)\n",
    "    pos  = alleles_sorted[pos]\n",
    "    for j in range(len(maxs)):\n",
    "        max_freq[pos[j]] = max(max_freq[pos[j]], maxs[j])\n",
    "        \n",
    "cutoff = 0.01\n",
    "\n",
    "idxs_keep = np.logical_and(types=='NS', max_freq>cutoff)\n",
    "\n",
    "alleles   = alleles[idxs_keep]\n",
    "selection = selection[idxs_keep]\n",
    "s_ind     = s_ind[idxs_keep]\n",
    "errors    = errors[idxs_keep]\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR, f'inf-data-nonsynonymous-{DATA_DATE}-1pct-test.npz'), allele_number=alleles, \n",
    "                    selection=selection, error_bars=errors, selection_independent=s_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13189\n",
      "13189\n",
      "13189\n",
      "13189\n",
      "13189\n",
      "13189\n"
     ]
    }
   ],
   "source": [
    "# Eliminate mutations that don't appear at least five times\n",
    "data = np.load(os.path.join(INF_DIR, f'inf-data-nonsynonymous-{DATA_DATE}-1pct-test.npz'), allow_pickle=True)\n",
    "muts = data['allele_number']\n",
    "s    = data['selection']\n",
    "err  = data['error_bars']\n",
    "s_ind= data['selection_independent']\n",
    "\n",
    "count_data = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "counts     = count_data['counts']\n",
    "alleles    = count_data['allele_number']\n",
    "\n",
    "mask1      = np.isin(alleles, muts)\n",
    "alleles    = alleles[mask1]\n",
    "counts     = counts[mask1]\n",
    "\n",
    "mask2      = np.isin(muts, alleles)\n",
    "muts       = muts[mask2]\n",
    "s          = s[mask2]\n",
    "s_ind      = s_ind[mask2]\n",
    "errors     = err[mask2]\n",
    "\n",
    "mask3      = np.nonzero(counts>5)[0]\n",
    "muts       = muts[mask3]\n",
    "s          = s[mask3]\n",
    "s_ind      = s_ind[mask3]\n",
    "errors     = errors[mask3]\n",
    "        \n",
    "np.savez_compressed(os.path.join(INF_DIR, f'inf-data-nonsynonymous-{DATA_DATE}-1pct-cutoff.npz'), allele_number=muts, \n",
    "                    selection=s, error_bars=errors, selection_independent=s_ind)\n",
    "\n",
    "dp.website_file2(os.path.join(INF_DIR, f'inf-data-nonsynonymous-{DATA_DATE}-1pct-cutoff.npz'),\n",
    "                 os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                 os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                 os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate sites due to issue with alignment\n",
    "bad_sites = ['28273-G', '28275-C', '28279-A', '28280--', '28281--']\n",
    "df        = pd.read_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn.csv'), index_col=False)\n",
    "nuc_nums  = list(df['nucleotide number'])\n",
    "nucs      = list(df['nucleotide'])\n",
    "labels    = [str(nuc_nums[i]) + '-' + nucs[i] for i in range(len(nucs))]\n",
    "idxs_drop = [labels.index(i) for i in bad_sites]\n",
    "df        = df.drop(index=idxs_drop)\n",
    "df.to_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change nucleotide indices to be indexed starting with 1 instead of 0 for publishing\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn.csv'), index_col=False)\n",
    "nucs = np.array(list(df['nucleotide number'])) + 1\n",
    "df['nucleotide number'] = nucs\n",
    "df.to_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn-paper.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\tn dupes\ttotal s\n",
      "NSP6-S106-\t3\t0.054\n",
      "NSP6-G107-\t3\t0.042\n",
      "NSP6-F108-\t3\t0.031\n",
      "S-L141-\t3\t0.031\n",
      "S-G142-\t3\t0.036\n",
      "ORF3a-V256-\t2\t0.035\n",
      "N-D3-\t4\t0.057\n",
      "N-S202-\t3\t0.035\n",
      "N-M234I\t3\t0.031\n"
     ]
    }
   ],
   "source": [
    "# Find mutations that are in the same codon and strongly linked to one another and sum their selection coefficients\n",
    "data  = np.load(os.path.join(DATA_DIR, 'linked-sites.npy'), allow_pickle=True)\n",
    "nucs  = [np.array([data[i][j][-1] for j in range(len(data[i]))]) for i in range(len(data))]\n",
    "idxs  = []\n",
    "for i in range(len(data)):\n",
    "    if len(np.array(nucs[i])[nucs[i]=='-']) != len(nucs[i]):\n",
    "        idxs.append(i)\n",
    "data = data[idxs]\n",
    "#for i in data:\n",
    "    #print(np.sort(i))\n",
    "split = [[data[i][j].split('-') for j in range(len(data[i]))] for i in range(len(data))]\n",
    "cods  = [[split[i][j][1] for j in range(len(split[i]))] for i in range(len(split))]\n",
    "prots = [[split[i][j][0] for j in range(len(split[i]))] for i in range(len(split))]\n",
    "    \n",
    "#linked_same_codon = []\n",
    "linked_groups     = []\n",
    "for i in range(len(data)):\n",
    "    group   = data[i]\n",
    "    #g_nucs  = nucs[i]\n",
    "    g_cods  = cods[i]\n",
    "    g_prots = prots[i]\n",
    "    prot_unique = np.unique(g_prots)\n",
    "    for j in range(len(prot_unique)):\n",
    "        temp_cods   = np.array(g_cods)[np.array(g_prots)==prot_unique[j]]\n",
    "        temp_labs   = np.array(group)[np.array(g_prots)==prot_unique[j]]\n",
    "        cods_unique = np.unique(temp_cods)\n",
    "        for k in range(len(cods_unique)):\n",
    "            labs_same_cod = temp_labs[temp_cods==cods_unique[k]]\n",
    "            if len(labs_same_cod)>1:\n",
    "                linked_groups.append(labs_same_cod)\n",
    "                #for l in labs_same_cod:\n",
    "                    #linked_same_codon.append(l)\n",
    "#linked_same_codon = [i for i in linked_same_codon if i[-1]!='-']\n",
    "linked_groups     = [[linked_groups[i][j] for j in range(len(linked_groups[i])) if linked_groups[i][j][-1]!='-'] for i in range(len(linked_groups))]\n",
    "linked_groups     = [i for i in linked_groups if len(i)!=0]\n",
    "linked_all        = [linked_groups[i][j] for i in range(len(linked_groups)) for j in range(len(linked_groups[i]))]\n",
    "\n",
    "\n",
    "# collapse identical mutations and sum contributions\n",
    "df_sel = pd.read_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn.csv'), memory_map=True)\n",
    "\n",
    "f = open(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn-collapsed.csv'), 'w')\n",
    "\n",
    "headers = list(df_sel.columns)\n",
    "f.write('%s\\n' % ','.join(headers))\n",
    "\n",
    "print('name\\tn dupes\\ttotal s')\n",
    "\n",
    "dup_names = []\n",
    "for iter, entry in df_sel.iterrows():\n",
    "    if entry['protein']=='NC':\n",
    "        continue\n",
    "    loc      = str(int(entry['amino acid number in protein']))\n",
    "    anc      = entry['amino acid mutation'][0]\n",
    "    mut      = entry['amino acid mutation'][-1]\n",
    "    var_name = str(entry['protein']) + '-' + anc + loc + mut\n",
    "    label    = dp.get_label2(str(entry['nucleotide number']) + '-' + entry['nucleotide'])\n",
    "    if var_name in dup_names:\n",
    "        continue\n",
    "    \n",
    "    df_same = df_sel[(df_sel['protein']==entry['protein']) \n",
    "                     & (df_sel['amino acid number in protein']==entry['amino acid number in protein'])\n",
    "                     & (df_sel['amino acid mutation']==entry['amino acid mutation'])]\n",
    "    \n",
    "    s = entry['selection coefficient']\n",
    "    if len(df_same)>1:\n",
    "        s = np.sum(df_same['selection coefficient'])\n",
    "        if var_name[-1]!='-' and label not in linked_all:\n",
    "            s = np.max(df_same['selection coefficient'])\n",
    "        dup_names.append(var_name)\n",
    "        if np.fabs(s)>0.03:\n",
    "            print('%s\\t%d\\t%.3f' % (var_name, len(df_same), s))\n",
    "        \n",
    "    for i in range(len(headers)):\n",
    "        if headers[i]=='selection coefficient':\n",
    "            f.write('%s' % str(s))\n",
    "        elif headers[i]=='linked sites':\n",
    "            f.write('\"%s\"' % str(entry[headers[i]]))\n",
    "#         elif headers[i]=='nucleotide number':\n",
    "#             f.write('%s' % entry[headers[i]][:-2])\n",
    "        else:\n",
    "            f.write('%s' % str(entry[headers[i]]))\n",
    "        if i!=len(headers)-1:\n",
    "            f.write(',')\n",
    "    f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change nucleotide indices to be indexed starting with 1 instead of 0 for publishing\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn-collapsed.csv'), index_col=False)\n",
    "nucs = np.array(list(df['nucleotide number'])) + 1\n",
    "df['nucleotide number'] = nucs\n",
    "df.to_csv(os.path.join(DATA_DIR, f'selection-g40-1pct-nonsyn-collapsed-paper.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring selection coefficients using a cutoff of 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.1 | wc -l\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-covar-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-freqs-0.1.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-multisite-0.1.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-freqs-0.1.sh &&\n",
      "sbatch job-infer-multisite-0.1.sh\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/infer-2021-08-14-cutoff0.1.npz /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "# infering the selection coefficients using a cutoff frequency of 10% and for different values of the smoothing window\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'freq_0.1')\n",
    "freq_script   = 'epi-covar-parallel.py'\n",
    "inf_script    = 'epi-inf-parallel.py'\n",
    "archive_loc   = os.path.join(SCRIPT_DIR, 'Archive-vector')\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'freqs-0.1')\n",
    "out_file      = os.path.join(SSH_DATA, f'infer-{DATA_DATE}-cutoff0.1')\n",
    "\n",
    "temp_dir2      = os.path.join(SSH_DATA, 'data', 'freqs-0.1-window10')\n",
    "out_file2      = os.path.join(SSH_DATA, f'infer-{DATA_DATE}-cutoff0.1-window10')\n",
    "temp_dir3      = os.path.join(SSH_DATA, 'data', 'freqs-0.1-window20')\n",
    "out_file3      = os.path.join(SSH_DATA, f'infer-{DATA_DATE}-cutoff0.1-window20')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 160    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-freqs-0.1.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=50G\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq10-error-%a\n",
    "#SBATCH -o ./MPL/out/freq10-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "cd ./Archive-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH}\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "job_file2     = 'job-find-freqs-window10.sh'\n",
    "job_str2      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=50G\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq10-error-%a\n",
    "#SBATCH -o ./MPL/out/freq10-out-%a\n",
    "#SBATCH -p batch\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "cd ./Archive-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir2}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH} --window 10\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "job_file3     = 'job-find-freqs-window20.sh'\n",
    "job_str3      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=50G\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq10-error-%a\n",
    "#SBATCH -o ./MPL/out/freq10-out-%a\n",
    "#SBATCH -p batch\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "cd ./Archive-vector\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir3}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH} --window 20\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file3), 'w')\n",
    "f.write('%s\\n' % job_str3)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Inference\n",
    "job_file4 = 'job-infer-0.1.sh'\n",
    "job_str4  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --time=1-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf2-error\n",
    "#SBATCH -o ./MPL/out/inf2-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {inf_script} --data {temp_dir}  --timed 1 -o {out_file}  -q 5 --g1 40 && \\\n",
    "python {inf_script} --data {temp_dir2} --timed 1 -o {out_file2} -q 5 --g1 40 && \\\n",
    "python {inf_script} --data {temp_dir3} --timed 1 -o {out_file3} -q 5 --g1 40\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file4), 'w')\n",
    "f.write('%s\\n' % job_str4)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file2, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file3, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file4, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file1)\n",
    "print('sbatch %s &&' % job_file2)\n",
    "print('sbatch %s &&' % job_file3)\n",
    "print('sbatch %s' % job_file4)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print('scp %s%s %s &&' % (SSH_HOME, out_file  + '.npz', INF_DIR))\n",
    "print('scp %s%s %s &&' % (SSH_HOME, out_file2 + '.npz', INF_DIR))\n",
    "print('scp %s%s %s'    % (SSH_HOME, out_file3 + '.npz', INF_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "selection data filtered\n"
     ]
    }
   ],
   "source": [
    "cutoff    = 0\n",
    "\n",
    "data      = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "muts      = data['allele_number']\n",
    "counts    = data['counts']\n",
    "\n",
    "regs      = [40]\n",
    "for reg in regs:\n",
    "\n",
    "    inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-cutoff0.1.npz'), allow_pickle=True)\n",
    "    alleles   = inf_data['allele_number']\n",
    "    selection = inf_data['selection']\n",
    "    errors    = inf_data['error_bars']\n",
    "    s_ind     = inf_data['selection_independent']\n",
    "    \n",
    "    mask   = np.isin(muts, alleles)\n",
    "    muts   = muts[mask]\n",
    "    counts = counts[mask]\n",
    "    \n",
    "    #numerator = inf_data['numerator']\n",
    "    #covar_int = inf_data['covar_int']\n",
    "    locations = inf_data['locations']\n",
    "    times     = inf_data['times']\n",
    "\n",
    "    print('data loaded')\n",
    "    clip_start= 150\n",
    "    nucs      = np.array([i[:-2] for i in alleles], dtype=int)\n",
    "\n",
    "    mask      = np.nonzero(np.logical_and(np.logical_and(counts>cutoff, selection!=0), nucs > clip_start))[0]\n",
    "\n",
    "    selection = selection[mask]\n",
    "    alleles   = alleles[mask]\n",
    "    errors    = errors[mask]\n",
    "    s_ind     = s_ind[mask]\n",
    "\n",
    "    print('selection data filtered')\n",
    "    np.savez_compressed(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-10pct-observed.npz'),\n",
    "                       selection=selection, selection_independent=s_ind, error_bars=errors,\n",
    "                       allele_number=alleles)\n",
    "    \n",
    "    dp.website_file2(os.path.join(DATA_DIR, 'sensitivity-data', f'infer-{DATA_DATE}-g-{reg}-10pct-observed.npz'),\n",
    "                     os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                     os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                     os.path.join(DATA_DIR, f'selection-g-40-1pct-observed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sampling directory\n",
    "sample_dir = os.path.join(DATA_DIR, 'sampling-data')\n",
    "input_dir  = os.path.join(DATA_DIR, 'genome-trimmed')\n",
    "dp.make_sampling_dir(sample_dir, input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the selection coefficients in the United Kingdom including migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/SARS-CoV-2-Data/2021-08-14/estimated-pop-size-uk.npy blee098@cluster.hpcc.ucr.edu://rhome/blee098/bigdata/SARS-CoV-2-Data\n",
      "scp -r /Users/brianlee/SARS-CoV-2-Data/2021-08-14/genome-uk blee098@cluster.hpcc.ucr.edu://rhome/blee098/bigdata/SARS-CoV-2-Data\n"
     ]
    }
   ],
   "source": [
    "# Putting uk files in their own directory\n",
    "uk_filepath = []\n",
    "uk_filename = []\n",
    "for file in sorted(os.listdir(os.path.join(DATA_DIR, 'freqs'))):\n",
    "    file_split = file.split('-')\n",
    "    if file_split[1] == 'united kingdom' and file_split[2] == 'england_wales_scotland':\n",
    "        year_end  = int(file_split[-3])\n",
    "        month_end = int(file_split[-2])\n",
    "        day_end   = int(file_split[-1][:-4])  \n",
    "        if (dt.date(year_end, month_end, day_end) - dt.date(2021, 5, 5)).days < 0:  # take only data before the middle of may\n",
    "            uk_filepath.append(os.path.join(DATA_DIR, 'freqs', file))\n",
    "            uk_filename.append(file)\n",
    "uk_dir = os.path.join(DATA_DIR, 'genome-uk')\n",
    "if not os.path.exists(uk_dir):\n",
    "    os.mkdir(uk_dir)\n",
    "for path in uk_filepath:\n",
    "    shutil.copy(path, uk_dir)\n",
    "\n",
    "print(f'scp -r {uk_dir} {SSH_HOME}/{SSH_DATA}/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-uk.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-infer-uk.sh &&\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:infer-2021-08-14--2021-05-01-uk-g-10.npz /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "# Finding selection coefficients in the uk only (up to may 1st 2021)\n",
    "out_file   = f'infer-{DATA_DATE}--2021-05-01-uk-g-10'\n",
    "data       = os.path.join(SSH_DATA, 'data', 'genome-uk')\n",
    "inf_script = 'epi-inf-parallel.py'\n",
    "\n",
    "job_file  = 'job-infer-uk.sh'\n",
    "job_str   = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf-error\n",
    "#SBATCH -o ./MPL/out/inf-out\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "outfile={out_file}\n",
    "python {inf_script} --data {data} --timed 1 -o $outfile -q 5 --g1 10\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file)\n",
    "print('')\n",
    "\n",
    "print('scp %s%s %s' % (SSH_HOME, out_file + '.npz', INF_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[0 0 0 ... 4 4 1]\n"
     ]
    }
   ],
   "source": [
    "# sites linked to S-222 mutation\n",
    "NUC = ['-', 'A', 'C', 'G', 'T']\n",
    "infer_data = np.load(os.path.join(INF_DIR, out_file + '.npz'), allow_pickle=True)\n",
    "allele_num = infer_data['allele_number']\n",
    "labels     = [dp.get_label(i[:-2]) + '-' + i[-1] for i in allele_num]\n",
    "\n",
    "freq_data  = np.load(os.path.join(DATA_DIR, 'freq_0.05', \n",
    "                     'europe-united kingdom-england_wales_scotland-2020-01-01-2021-01-01.n---2020-2-24-2021-1-1.npz'),\n",
    "                     allow_pickle=True)\n",
    "nVec       = freq_data['nVec']\n",
    "sVec       = freq_data['sVec']\n",
    "times      = freq_data['times']\n",
    "mutants    = freq_data['mutant_sites']\n",
    "mask       = np.nonzero(mutants<29700)[0]\n",
    "#mask       = np.nonzero(np.logical_and(mutants<29700), mutants>150)[0]\n",
    "mutants    = mutants[mask]\n",
    "sVec_temp  = []\n",
    "for t in sVec:\n",
    "    sVec_t = []\n",
    "    for i in t:\n",
    "        sVec_t.append(i[mask])\n",
    "    sVec_temp.append(sVec_t)\n",
    "sVec = sVec_temp\n",
    "nuc_nums   = [dp.get_label(i) for i in mutants]\n",
    "loc_sites  = []\n",
    "for site in mutants:\n",
    "    for j in range(5):\n",
    "        loc_sites.append(dp.get_label(site) + '-' + NUC[j])\n",
    "\n",
    "link_sites = ['ORF10-30-0-T', 'N-220-1-T', 'M-93-2-G', 'S-222-1-T', 'NSP16-199-2-C', 'NSP3-1189-2-T', 'NSP1-60-2-C']    # The linked mutations that will inflow\n",
    "sites      = [allele_num[labels.index(i)] for i in link_sites]\n",
    "\n",
    "# Find a sequence containing the desired list of mutations\n",
    "inflow_seq = None\n",
    "seq_found  = False\n",
    "for i in range(len(sVec)):\n",
    "    for j in range(len(sVec[i])):\n",
    "        muts = True\n",
    "        for k in range(len(link_sites)):\n",
    "            if sVec[i][j][nuc_nums.index(link_sites[k][:-2])] != NUC.index(link_sites[k][-1]):\n",
    "                muts = False\n",
    "        if muts:\n",
    "            inflow_seq = sVec[i][j]\n",
    "            seq_found  = True\n",
    "            break\n",
    "    if seq_found:\n",
    "        break\n",
    "print(inflow_seq)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zipped file containing the estimated population size\n",
    "est_dir = os.path.join(DATA_DIR, 'ihme-popsize-2021_09_01')\n",
    "import zipfile\n",
    "zipped = os.path.join(est_dir, 'reference_hospitalization_all_locs.csv.zip')\n",
    "with zipfile.ZipFile(zipped, 'r') as zip_ref:\n",
    "    zip_ref.extractall(est_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-5241535f716a>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-5241535f716a>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    est_dir = os.path.join(DATA_DIR, 'ihme-popsize-2021_09_01')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Running the inference with different numbers of infected individuals migrating into the United Kingdom\n",
    "    \n",
    "file1   = os.path.join(est_dir, 'reference_hospitalization_all_locs.csv')\n",
    "data1   = pd.read_csv(file1)\n",
    "popsize = list(data1.est_infections_mean)\n",
    "dates   = list(data1.date)\n",
    "locs    = [i.lower() for i in list(data1.location_name)]\n",
    "uk_popsize  = np.array(popsize)[np.array(locs)=='united kingdom']\n",
    "uk_dates    = np.array(dates)[np.array(locs)=='united kingdom']\n",
    "uk_dates    = uk_dates[~np.isnan(uk_popsize)]\n",
    "uk_times    = [(dt.date.fromisoformat(uk_dates[i]) - dt.date(2020,1,1)).days for i in range(len(uk_dates))]\n",
    "uk_popsize  = uk_popsize[~np.isnan(uk_popsize)]\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}--2021-05-01-uk-g-10' + '.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "numerator = inf_data['numerator']\n",
    "covar_int = inf_data['covar_int']\n",
    "traj      = inf_data['traj']\n",
    "mutants   = inf_data['mutant_sites']\n",
    "locs      = inf_data['locations']\n",
    "dates     = inf_data['times']\n",
    "idx       = list(locs).index('europe-united kingdom-england_wales_scotland-2020-01-01-2021-01-01.n---2020-2-24-2021-1-1')\n",
    "traj_uk   = traj[idx]\n",
    "muts_uk   = mutants[idx]\n",
    "time      = dates[idx]\n",
    "\n",
    "q   = 5\n",
    "N   = 10000\n",
    "k   = 0.1\n",
    "R   = 2\n",
    "g   = 10\n",
    "g  *= (N * k) / (1 + (k / R))\n",
    "\n",
    "for i in range(len(covar_int)):\n",
    "    covar_int[i, i] += g\n",
    "    \n",
    "start_t    = 188    # number of days after february 24th 2020 that the individuals infected with 20E-EU1 start migrating\n",
    "end_t      = 288\n",
    "T          = end_t - start_t + 1\n",
    "uk_popsize = uk_popsize[uk_times.index(start_t):uk_times.index(end_t)+1]\n",
    "traj_uk    = traj_uk[list(time).index(start_t):list(time).index(end_t)+1]\n",
    "\n",
    "ref_seq, ref_tag  = dp.get_MSA(REF_TAG +'.fasta')\n",
    "ref_seq       = list(ref_seq[0])\n",
    "allele_number = pd.unique([int(i[:-2]) for i in alleles])\n",
    "mut_number    = pd.unique([int(i[:-2]) for i in muts_uk])\n",
    "ref_poly      = np.array(ref_seq)[allele_number]\n",
    "\n",
    "alleles_sorted = np.argsort(allele_number)\n",
    "positions      = np.searchsorted(allele_number[alleles_sorted], mut_number)\n",
    "\n",
    "counts     = [0, 4, 8, 10, 20, 50, 100, 500, 1000]\n",
    "s_tot      = []\n",
    "for i in range(len(counts)):\n",
    "    count = counts[i]\n",
    "    inflow_seqs   = [[inflow_seq] for i in range(T)]\n",
    "    inflow_counts = [[count] for i in range(T)]\n",
    "    pop_in        = [np.sum(i) for i in inflow_counts]\n",
    "    inflow_term   = dp.allele_counter_in(inflow_seqs, inflow_counts, traj_uk, pop_in, uk_popsize, k, R, T, d=5)\n",
    "    inflow_tot    = np.zeros(len(alleles))\n",
    "    for j in range(len(mut_number)):\n",
    "        inflow_tot[positions[j] * 5 : (positions[j] + 1) * 5] += inflow_term[j * 5 : (j + 1) * 5]\n",
    "    A = covar_int\n",
    "    b = numerator - inflow_tot\n",
    "    \n",
    "    s = linalg.solve(A, b, assume_a='sym')\n",
    "\n",
    "    # normalize reference nucleotide to zero\n",
    "    L = int(len(s) / 5)\n",
    "    selection  = np.reshape(s, (L, 5))\n",
    "\n",
    "    s_new = []\n",
    "    for i in range(L):\n",
    "        idx = NUC.index(ref_poly[i])\n",
    "        temp_s    = selection[i]\n",
    "        temp_s    = temp_s - temp_s[idx]\n",
    "        s_new.append(temp_s)\n",
    "    selection = s_new\n",
    "    selection = np.array(selection).flatten()\n",
    "    s_tot.append(selection)\n",
    "    np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-inflow{count}.npz'), \n",
    "                        selection=selection, allele_number=allele_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "EU1 = [['NSP16-199-2-C', 'NSP1-60-2-C', 'NSP3-1189-2-T', 'M-93-2-G', 'N-220-1-T', 'ORF10-30-0-T', 'S-222-1-T']]\n",
    "np.save(os.path.join(DATA_DIR, '20E-EU1.npy'), EU1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(INF_DIR, 'inflow-uk')):\n",
    "    os.mkdir(os.path.join(INF_DIR, 'inflow-uk'))\n",
    "for count in counts:\n",
    "    shutil.copy(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-inflow{count}.npz'), \n",
    "                os.path.join(INF_DIR, 'inflow-uk', f'infer-{DATA_DATE}-uk-inflow{count}.npz'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(INF_DIR, 'inflow-uk')):\n",
    "    os.mkdir(os.path.join(INF_DIR, 'inflow-uk'))\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}--2021-05-01-uk-g-10' + '.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "counts     = [0, 4, 8, 10, 20, 50, 100, 500, 1000]\n",
    "for count in counts:\n",
    "    data = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-inflow{count}.npz'), allow_pickle=True)\n",
    "    np.savez_compressed(os.path.join(INF_DIR, 'inflow-uk', f'infer-{DATA_DATE}-uk-inflow{count}.npz'),\n",
    "                        selection=data, allele_number=alleles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15612642  0.15442725  0.15272809  0.1518785   0.14763058  0.13488683\n",
      "  0.11364723 -0.05626954 -0.26866549]\n",
      "number of migrating individuals per day in order for 20E-EU1 variant to be zero 367.53623582591405\n"
     ]
    }
   ],
   "source": [
    "sites    = [dp.get_label(i[:-2]) + '-' + i[-1] for i in alleles]\n",
    "var_idxs = np.array([sites.index(site) for site in link_sites])\n",
    "s_var    = np.zeros(len(s_tot))\n",
    "for i in range(len(var_idxs)):\n",
    "    for j in range(len(s_tot)):\n",
    "        s_var[j] += s_tot[j][var_idxs[i]]\n",
    "slope  = (s_var[4] - s_var[0]) / (counts[4] - counts[0])\n",
    "x_zero = - s_var[0] / slope\n",
    "print('number of migrating individuals per day in order for 20E-EU1 variant to be zero', x_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the effect of B.1.1.7 on selection coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ['NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--',\n",
    "         'NSP6-108-1--', 'NSP6-108-2--', 'S-501-0-T', 'NSP12-412-2-T', 'NSP2-36-2-T', 'NSP3-183-1-T', 'NSP3-890-1-A', \n",
    "         'NSP3-1089-2-T', 'NSP3-1412-1-C', 'NSP12-613-2-T', 'NSP12-912-2-C', 'S-68-1--', 'S-68-2--', 'S-69-0--', 'S-69-1--', \n",
    "         'S-69-2--', 'S-70-0--', 'S-143-2--', 'S-144-0--', 'S-144-1--', 'S-570-1-A', 'S-681-1-A', 'S-716-1-T', 'S-982-0-G', \n",
    "         'S-1118-0-C', 'ORF8-27-0-T', 'ORF8-52-1-T', 'ORF8-73-1-G', 'N-3-0-C', 'N-3-1-T', 'N-3-2-A', 'N-235-1-T',\n",
    "         'N-203-1-A', 'N-203-2-A', 'N-204-0-C']\n",
    "np.save(os.path.join(SARS_DIR, 'b117-muts.npy'), alpha)\n",
    "\n",
    "cutoff     = 0\n",
    "\n",
    "count_data = np.load(os.path.join(DATA_DIR, 'nucleotide-counts.npz'), allow_pickle=True)\n",
    "muts       = count_data['allele_number']\n",
    "counts     = count_data['counts']\n",
    "\n",
    "# Same as above but for a single file\n",
    "syn_data  = np.load(os.path.join(DATA_DIR, 'synonymous-prot-test.npz'), allow_pickle=True)\n",
    "data      = np.load(os.path.join(INF_DIR, f'inf-{DATA_DATE}', f'infer-{DATA_DATE}-g-40.npz'), allow_pickle=True)\n",
    "\n",
    "NUC = ['-', 'A', 'C', 'G', 'T']\n",
    "q   = 5\n",
    "N   = 10000\n",
    "k   = 0.1\n",
    "R   = 2\n",
    "g   = 40\n",
    "alleles   = data['allele_number']\n",
    "numerator = data['numerator']\n",
    "covar_int = data['covar_int']\n",
    "syn_muts  = syn_data['nuc_index']\n",
    "types     = syn_data['types']\n",
    "types     = types[np.isin(syn_muts, alleles)]\n",
    "g1 = np.array(g, dtype=float) * (N * k) / (1 + (k / R))\n",
    "for i in range(len(covar_int)):\n",
    "    covar_int[i, i] += g1\n",
    "    \n",
    "mask   = np.isin(muts, alleles)\n",
    "muts   = muts[mask]\n",
    "counts = counts[mask]\n",
    "    \n",
    "# Mask out sites that have mutations in alpha\n",
    "alpha_short = np.array([i[:-2] for i in alpha])\n",
    "muts_short  = np.array([dp.get_label2(i)[:-2] for i in alleles])\n",
    "mask        = np.isin(muts_short, alpha_short, invert=True)\n",
    "alleles     = alleles[mask]\n",
    "numerator   = numerator[mask]\n",
    "covar_int   = covar_int[mask][:, mask]\n",
    "types       = types[mask]\n",
    "counts      = counts[mask]\n",
    "\n",
    "# infer selection coefficients\n",
    "s      = linalg.solve(covar_int, numerator, assume_a='sym')\n",
    "s_ind  = numerator / np.diag(covar_int)\n",
    "errors = 1 / np.sqrt(np.absolute(np.diag(covar_int)))\n",
    "\n",
    "print('selection coefficients found')\n",
    "\n",
    "# normalize reference nucleotide to zero\n",
    "L = int(len(s) / 5)\n",
    "selection  = np.reshape(s, (L, q))\n",
    "selection_nocovar = np.reshape(s_ind, (L, q))\n",
    "ref_seq, ref_tag  = dp.get_MSA(REF_TAG +'.fasta')\n",
    "ref_seq  = list(ref_seq[0])\n",
    "allele_number = np.unique([int(i[:-2]) for i in alleles])\n",
    "ref_poly = np.array(ref_seq)[allele_number]\n",
    "\n",
    "s_new = []\n",
    "s_SL  = []\n",
    "for i in range(L):\n",
    "    idx = NUC.index(ref_poly[i])\n",
    "    temp_s    = selection[i]\n",
    "    temp_s    = temp_s - temp_s[idx]\n",
    "    temp_s_SL = selection_nocovar[i]\n",
    "    temp_s_SL = temp_s_SL - temp_s_SL[idx]\n",
    "    s_new.append(temp_s)\n",
    "    s_SL.append(temp_s_SL)\n",
    "selection         = s_new\n",
    "selection_nocovar = s_SL\n",
    "\n",
    "selection         = np.array(selection).flatten()\n",
    "selection_nocovar = np.array(selection_nocovar).flatten()\n",
    "error_bars        = errors\n",
    "\n",
    "# Eliminate reference nucleotides and mutations that aren't observed\n",
    "\n",
    "mask      = np.nonzero(np.logical_and(selection!=0, counts>cutoff))[0]\n",
    "alleles   = alleles[mask]\n",
    "selection = selection[mask]\n",
    "s_ind     = selection_nocovar[mask]\n",
    "errors    = error_bars[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9280751653e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     allele_number=alleles, traj=traj, mutant_sites=mutants)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m dp.website_file(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-{reg}-observed-mask-alpha.npz'),\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synonymous-test.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linked-sites.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "traj_data = np.load(os.path.join(DATA_DIR, 'trajectories-1pct.npz'), allow_pickle=True)\n",
    "traj      = traj_data['traj']\n",
    "mutants   = traj_data['mutant_sites']\n",
    "muts_new  = []\n",
    "for i in range(len(mutants)):\n",
    "    #idxs_keep  = np.array([j for j in range(len(mutants[i])) if mutants[i][j] in alleles])\n",
    "    idxs_keep  = np.isin(mutants[i], alleles)\n",
    "    muts_new.append(np.array(mutants[i])[idxs_keep])\n",
    "    traj[i]    = np.array([traj[i][:, j] for j in range(len(traj[i][0])) if j in idxs_keep])\n",
    "    traj[i]    = np.swapaxes(traj[i], 0, 1)\n",
    "mutants = muts_new\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-40-observed-mask-alpha.npz'),\n",
    "                    selection=selection, selection_independent=s_ind, error_bars=errors,\n",
    "                    allele_number=alleles, traj=traj, mutant_sites=mutants)\n",
    "    \n",
    "dp.website_file2(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-40-observed-mask-alpha.npz'),\n",
    "                 os.path.join(DATA_DIR, 'synonymous-prot-test.npz'),\n",
    "                 os.path.join(DATA_DIR, 'linked-sites.npy'), \n",
    "                 os.path.join(DATA_DIR, f'selection-g-40-observed-mask-alpha.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selection coefficient for 20E-EU1 when alpha is kept in the data:                                0.10225873676301382\n",
      "selection coefficient for 20E-EU1 when alpha mutations are reverted to the reference nucleotide: -0.01673100542328889\n"
     ]
    }
   ],
   "source": [
    "unmasked    = np.load(os.path.join(INF_DIR, f'inf-{DATA_DATE}', f'infer-{DATA_DATE}-g-40.npz'), allow_pickle=True)\n",
    "masked      = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-{g}-observed-mask-alpha.npz'), allow_pickle=True)\n",
    "s_unmask    = unmasked['selection']\n",
    "s_mask      = masked['selection']\n",
    "muts_mask   = [dp.get_label2(i) for i in masked['allele_number']]\n",
    "muts_unmask = [dp.get_label2(i) for i in unmasked['allele_number']]\n",
    "link_sites  = ['ORF10-30-0-T', 'N-220-1-T', 'M-93-2-G', 'S-222-1-T', 'NSP16-199-2-C', 'NSP3-1189-2-T', 'NSP1-60-2-C']\n",
    "\n",
    "sel_mask    = np.sum(s_mask[np.isin(muts_mask, link_sites)])\n",
    "sel_unmask  = np.sum(s_unmask[np.isin(muts_unmask, link_sites)])\n",
    "print('selection coefficient for 20E-EU1 when alpha is kept in the data:                               ', sel_unmask)\n",
    "print('selection coefficient for 20E-EU1 when alpha mutations are reverted to the reference nucleotide:', sel_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "74\n",
      "correlation of frequency changes: -0.2639345794687149\n",
      "1\n",
      "1\n",
      "correlation of frequency changes in UK: -0.7460799758819553\n",
      "2\n",
      "2\n",
      "correlation of frequency changes in UK: -0.7332002054076147\n"
     ]
    }
   ],
   "source": [
    "# Finding the correlation between frequency changes for B.1.1.7 and 20E-EU1\n",
    "traj_file = os.path.join(DATA_DIR, 'linked-trajectories-1pct.csv')\n",
    "\n",
    "print('correlation of frequency changes:', dp.freq_change_correlation(traj_file, 'alpha', '20E_EU1'))\n",
    "\n",
    "print('correlation of frequency changes in UK:', dp.freq_change_correlation(traj_file, 'alpha', '20E_EU1', \n",
    "                                                                            region='europe-united kingdom-england_wales_scotland-2020-01-01-2021-01-01.n---2020-2-24-2021-1-1'))\n",
    "\n",
    "print('correlation of frequency changes in UK:', dp.freq_change_correlation(traj_file, 'alpha', '20E_EU1', \n",
    "                                                                            region=['europe-united kingdom-england_wales_scotland-2020-01-01-2021-01-01.n---2020-2-24-2021-1-1',\n",
    "                                                                                    'europe-united kingdom-england_wales_scotland-2021-01-01-2021-05-01.n---2020-12-29-2021-5-1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Null Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(epi_figs)\n",
    "null = epi_figs.find_null_distribution_ind(os.path.join(INF_DIR, 'infer-2021-01-31-tv.npz'), \n",
    "                                           os.path.join(DATA_DIR, 'linked-sites.npy'),\n",
    "                                           os.path.join(SARS_DIR, 'individual-regions', 'inference')\n",
    "fig, axes = plt.subplots(1,1, figsize=[20,10])\n",
    "axes.hist(null, log=True, bins=50)\n",
    "\n",
    "f = open(os.path.join(SARS_DIR, 'null-distribution-new.csv'), mode='w')\n",
    "for i in range(len(null)):\n",
    "    f.write('%.8f\\n' % null[i])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Estimate of Omicron Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts    = ['NC-240-T', 'NSP3-38-1-G', 'NSP3-106-2-T', 'NSP3-889-2-G', 'NSP3-1265-1--', 'NSP3-1265-2--', \n",
    "           'NSP3-1266-0--', 'NSP3-1892-0-A', 'NSP4-492-1-T', 'NSP5-132-1-A', 'NSP6-104-1--', 'NSP6-104-2--', \n",
    "           'NSP6-105-0--', 'NSP6-105-1--', 'NSP6-105-2--', 'NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', \n",
    "           'NSP6-107-0--', 'NSP6-189-0-G', 'NSP10-57-2-C', 'NSP12-323-1-T', 'NSP12-600-2-T', 'NSP14-42-0-G', \n",
    "           'S-67-1-T', 'S-68-1--', 'S-68-2--', 'S-69-0--', 'S-69-1--', 'S-69-2--', 'S-70-0--', 'S-95-1-T', \n",
    "           'S-142-1--', 'S-142-2--', 'S-143-0--', 'S-143-1--', 'S-143-2--', 'S-144-0--', 'S-144-1--', \n",
    "           'S-144-2--', 'S-145-0--', 'S-211-1--', 'S-211-2--', 'S-212-0--', 'S-339-1-A', 'S-371-0-C', \n",
    "           'S-371-1-T', 'S-373-0-C', 'S-417-2-T', 'S-440-2-G', 'S-446-0-A', 'S-547-1-A', 'S-614-1-G', \n",
    "           'S-655-0-T', 'S-679-2-G', 'S-681-1-A', 'S-764-2-A', 'S-796-0-T', 'S-856-2-A', 'S-954-2-T', \n",
    "           'S-969-2-A', 'S-981-0-T', 'S-1146-2-T', 'ORF3a-64-2-T', 'E-9-1-T', 'M-3-1-G', 'M-19-0-G', \n",
    "           'M-63-0-A', 'ORF6-20-0-C', 'ORF7b-18-2-T', 'NC-28270-T', 'N-13-1-T', 'N-30-1--', 'N-30-2--', \n",
    "           'N-31-0--', 'N-31-1--', 'N-31-2--', 'N-32-0--', 'N-32-1--', 'N-32-2--', 'N-33-0--', 'N-203-1-A', \n",
    "           'N-203-2-A', 'N-204-0-C']    # Omicron mutations (excluding the insertion, which doesn't appear in our data)\n",
    "data    = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-g-40-1pct-observed.npz'), allow_pickle=True)\n",
    "alleles = [dp.get_label2(i) for i in data['allele_number']]\n",
    "s       = data['selection']\n",
    "s_tot   = np.sum(s[np.isin(alleles, muts)])\n",
    "print(f'selection for omicron variant is {s_tot}')\n",
    "print(f'number of mutations showing up in the old data is {len(s[np.isin(alleles, muts)])} out of {len(muts) + 9} total mutations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Early detection in the UK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/uk | wc -l\n",
      "mkdir /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/uk\n",
      "cp \"/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/freq_0.05/europe-united kingdom-england_wales_scotland*\" \"/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14/uk\"\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-covar-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel-tv.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive-tv-int blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-find-freqs-tv.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-tv.sh blee098@cluster.hpcc.ucr.edu:.\n",
      "\n",
      "sbatch job-find-freqs-tv.sh &&\n",
      "sbatch job-infer-tv.sh\n",
      "mkdir /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv\n",
      "mv /rhome/blee098/bigdata/SARS-CoV-2-Data/infer-tv-uk-2021-08-14---* /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv\n",
      "\n",
      "scp blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/infer-tv-uk-2021-08-14.npz /Users/brianlee/Python/MPL/6-29-20-epidemiological\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "# infering the selection coefficients using a cutoff frequency of 5% in the UK\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'uk')\n",
    "freq_script   = 'epi-covar-parallel.py'\n",
    "archive_loc   = os.path.join(SCRIPT_DIR, 'Archive-tv-int')\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'freqs-tv')\n",
    "uk_dir        = os.path.join(SSH_DATA, 'data', 'uk')\n",
    "freqs_dir     = os.path.join(SSH_DATA, 'data', 'freq_0.05')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print(f'mkdir {uk_dir}')\n",
    "print(f'cp \\\"{freqs_dir}/europe-united kingdom-england_wales_scotland*\\\" \\\"{uk_dir}\\\"')\n",
    "print('')\n",
    "\n",
    "num_files     = 3    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-freqs-tv.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=250G\n",
    "#SBATCH --time=7-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq-tv-error-%a\n",
    "#SBATCH -o ./MPL/out/freq-tv-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "cd ./Archive-tv-int\n",
    "g++ src/main-test.cpp src/inf-test.cpp src/io-test.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH} --timed 1 --tv_inference --delta_t 10\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s &&' % job_file1)\n",
    "print('sbatch %s' % job_file2)\n",
    "print(f'mkdir {SSH_DATA}/inf-{DATA_DATE}-uk-tv')\n",
    "print(f'mv {out_file}---* {SSH_DATA}/inf-{DATA_DATE}-uk-tv')\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print(f'scp -r {SSH_HOME}{temp_dir} {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_freqs = os.path.join(DATA_DIR, 'freqs-tv')\n",
    "t_start = []\n",
    "t_end   = []\n",
    "for file in os.listdir(local_freqs):\n",
    "    if os.path.isfile(os.path.join(local_freqs, file)):\n",
    "        times = np.load(os.path.join(local_freqs, file), allow_pickle=True)['times_full']\n",
    "        t_start.append(times[0])\n",
    "        t_end.append(times[-1])\n",
    "t_start = np.amin(t_start)\n",
    "t_end   = np.amax(t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560\n",
      "69 79 89 99 109 119 129 139 149 159 169 179 189 199 209 219 229 239 249 259 269 279 289 299 309 319 329 339 349 359 369 379 389 399 409 419 429 439 449 459 469 479 489 499 509 519 529 539 549 559 569\n",
      "mkdir /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv2\n",
      "mv /rhome/blee098/bigdata/SARS-CoV-2-Data/infer-tv-uk-2021-08-14-new---* /rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv2\n",
      "\n",
      "scp /Users/brianlee/Python/MPL/epi-inf-parallel-tv.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r /Users/brianlee/Python/MPL/6-29-20-epidemiological/Archive-tv-int blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-infer-tv4.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/inf-2021-08-14-uk-tv2 /Users/brianlee/Python/MPL/6-29-20-epidemiological\n"
     ]
    }
   ],
   "source": [
    "inf_script    = 'epi-inf-parallel-tv.py'\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'freqs-tv')\n",
    "out_file      = os.path.join(SSH_DATA, f'infer-tv-uk-{DATA_DATE}')\n",
    "\n",
    "# Break into time intervals so inference on different dates can be run in parallel\n",
    "start_times   = np.arange(t_start,     t_end - 10, 10)   \n",
    "end_times     = np.arange(t_start + 9, t_end - 1,  10)\n",
    "\n",
    "s_times       = ' '.join([str(i) for i in start_times])\n",
    "e_times       = ' '.join([str(i) for i in end_times])\n",
    "print(s_times)    # copy paste this under s_times in the job_string\n",
    "print(e_times)    # copy paste this under e_times in the job_string\n",
    "\n",
    "job_file2 = 'job-infer-tv.sh'\n",
    "job_str2  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=50G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf-tv2-error-%a\n",
    "#SBATCH -o ./MPL/out/inf-tv2-out-%a\n",
    "#SBATCH --array=0-{len(start_times) - 1}\n",
    "s_times=(60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560)\n",
    "e_times=(69 79 89 99 109 119 129 139 149 159 169 179 189 199 209 219 229 239 249 259 269 279 289 299 309 319 329 339 349 359 369 379 389 399 409 419 429 439 449 459 469 479 489 499 509 519 529 539 549 559 569)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "init_time=${{s_times[$SLURM_ARRAY_TASK_ID]}}\n",
    "final_time=${{e_times[$SLURM_ARRAY_TASK_ID]}}\n",
    "python {inf_script} --data {temp_dir} --timed 1 -o {out_file} -q 5 --g1 10 --delta_t 10 --start_time $init_time --end_time $final_time --no_traj\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "# run on cluster\n",
    "print(f'mkdir {SSH_DATA}/inf-{DATA_DATE}-uk-tv')\n",
    "print(f'mv {out_file}---* {SSH_DATA}/inf-{DATA_DATE}-uk-tv')\n",
    "print('')\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file2, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# transfer inference files to local directory\n",
    "print(f'scp -r {SSH_HOME}{SSH_DATA}/inf-{DATA_DATE}-uk-tv {INF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 36610)\n"
     ]
    }
   ],
   "source": [
    "alleles    = []\n",
    "selection  = []\n",
    "times_temp = []\n",
    "for file in os.listdir(os.path.join(INF_DIR, f'inf-{DATA_DATE}-uk-tv')):\n",
    "    times_temp.append(int(file.split('---')[-1][:-4]))\n",
    "times = np.arange(np.amin(times_temp), np.amax(times_temp) + 1)\n",
    "\n",
    "for i in times:\n",
    "    data_temp = np.load(os.path.join(INF_DIR, f'inf-{DATA_DATE}-uk-tv', f'infer-tv-uk-{DATA_DATE}---{i}.npz'))\n",
    "    if 'allele_number' in data_temp:\n",
    "        alleles = data_temp['allele_number']\n",
    "    selection.append(data_temp['selection'].flatten())\n",
    "np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-tv.npz'), selection=selection, allele_number=alleles, times=times)\n",
    "inf_times = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find uk frequency trajectories\n",
    "input_dir     = os.path.join(SSH_DATA, 'data', 'uk')\n",
    "freq_script   = 'epi-traj-parallel.py'\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data', 'freqs-uk')\n",
    "\n",
    "# run on cluster\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 3    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-freqs-uk.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=150G\n",
    "#SBATCH --time=5-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq-uk-error-%a\n",
    "#SBATCH -o ./MPL/out/freq-uk-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "\n",
    "files=({input_dir}/*)\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python {freq_script} --data \\\"$file\\\" -o $tempout -q 5 --timed 1 --window 10\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print(f'mkdir {temp_dir}')\n",
    "print('sbatch %s &&' % job_file1)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print(f'scp -r {SSH_HOME}{temp_dir} {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26715,)\n",
      "(309, 26715)\n",
      "(27630,)\n",
      "(81, 27630)\n",
      "(33175,)\n",
      "(120, 33175)\n",
      "['europe-united kingdom-england_wales_scotland-2020-01-01-2021-01-01.n---2020-2-24-2021-1-1', 'europe-united kingdom-england_wales_scotland-2021-05-01-2021-09-01.n---2021-5-1-2021-7-24', 'europe-united kingdom-england_wales_scotland-2021-01-01-2021-05-01.n---2020-12-29-2021-5-1']\n"
     ]
    }
   ],
   "source": [
    "mutant_sites = []\n",
    "traj         = []\n",
    "traj_full    = []\n",
    "locs         = []\n",
    "times_full   = []\n",
    "times        = []\n",
    "locations    = []\n",
    "for file in os.listdir(os.path.join(DATA_DIR, 'freqs-uk')):\n",
    "    loc  = file[:-4]\n",
    "    data = np.load(os.path.join(DATA_DIR, 'freqs-uk', file), allow_pickle=True)\n",
    "    traj.append(data['traj'])\n",
    "    mutant_sites.append(data['allele_number'])\n",
    "    traj_full.append(data['traj_nosmooth'])\n",
    "    times_full.append(data['times_full'])\n",
    "    times.append(data['times'])\n",
    "    locations.append(loc)\n",
    "    traj_times = data['times']\n",
    "    if inf_times[-1] in traj_times:\n",
    "        traj_times = traj_times[:list(traj_times).index(inf_times[-1])]\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR, 'uk-trajectories.npz'), traj=traj, \n",
    "                    traj_nosmooth=traj_full, times=times, times_full=times_full, mutant_sites=mutant_sites,\n",
    "                    locations=locations)\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-tv.npz'))\n",
    "selection = inf_data['selection']\n",
    "alleles   = inf_data['allele_number']\n",
    "times_inf = inf_data['times']\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-tv-traj.npz'),\n",
    "                    selection=selection, allele_number=alleles, traj=traj, mutant_sites=mutant_sites, traj_nosmooth=traj_full,\n",
    "                    times=times, times_full=times_full, times_inf=times_inf, locations=locations)\n",
    "times = times_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twentyE_EU1 = ['NSP16-199-2-C', 'NSP1-60-2-C', 'NSP3-1189-2-T', 'M-93-2-G', 'N-220-1-T', 'ORF10-30-0-T', 'S-222-1-T']    # Done\n",
    "\n",
    "delta       = ['NSP12-671-0-A', 'NC-209-T', 'NSP13-77-1-T', 'S-19-1-G', 'S-156-1--', 'S-156-2--', 'S-157-0--', 'S-157-1--', 'S-157-2--', \n",
    "               'S-158-0--', 'S-478-1-A', 'S-950-0-A', 'ORF3a-26-1-T', 'M-82-1-C', 'ORF7a-82-1-C', 'ORF7a-120-1-T', \n",
    "               'ORF8-119-0--', 'ORF8-119-1--', 'ORF8-119-2--', 'ORF8-120-0--', 'ORF8-120-1--', 'ORF8-120-2--', 'N-63-1-G', \n",
    "               'N-377-0-T', 'S-452-1-G', 'S-681-1-G']  \n",
    "# Eliminated 'N-203-1-T' and 'NC-28270--' \n",
    "\n",
    "tv_inf_file = os.path.join(INF_DIR, f'infer-{DATA_DATE}-uk-tv-traj.npz')\n",
    "out_file    = os.path.join(DATA_DIR, 'delta-tv-s')\n",
    "dp.save_traj_selection(tv_inf_file, group=delta, traj_site='S-19-1-G', out_file=out_file)\n",
    "\n",
    "out_file    = os.path.join(DATA_DIR, '20E-EU1-tv-s')\n",
    "dp.save_traj_selection(tv_inf_file, group=twentyE_EU1, traj_site='S-222-1-T', out_file=out_file)\n",
    "\n",
    "out_file    = os.path.join(DATA_DIR, 'S-A222V-tv-s')\n",
    "dp.save_traj_selection(tv_inf_file, group=['S-222-1-T'], traj_site='S-222-1-T', out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Early Detection in London__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_dir   = os.path.join(SSH_DATA, 'data' + '-lond')\n",
    "london_local = DATA_DIR + '-lond'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting only the data in the United Kingdom in London\n",
    "identifier_uk = f'regions-{DATA_DATE}-lond'\n",
    "\n",
    "selected_uk   =  [['europe', 'united kingdom', 'england', 'lond', None, None]]\n",
    "\n",
    "f = open(os.path.join(SARS_DIR, identifier_uk + '.npy'), mode='w+b')\n",
    "np.save(f, selected_uk)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Transfer scripts, run processing, and extract files\n",
    "# Processing clips the time series in different regions according to set rules regarding the distribution of the number of sampled genomes over time\n",
    "data_module   = 'data_processing.py'\n",
    "data_mod_path = os.path.join(SCRIPT_DIR, data_module)\n",
    "out_folder    = os.path.join(SSH_DATA, 'data' + '-lond')\n",
    "input_file    = 'merged-final.csv'\n",
    "script_file   = 'processing-multiallele.py'\n",
    "\n",
    "job_file      = 'job-processing-uk.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=10:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/genome-uk-error\n",
    "#SBATCH -o ./MPL/out/genome-uk-out\n",
    "#SBATCH -p highmem\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_file {os.path.join(SSH_DATA, input_file)} -o {out_folder} --regions {identifier_uk + '.npy'} --find_syn_off --no_trim\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "print('scp %s/%s %s. &&' % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SCRIPT_DIR, data_module, SSH_HOME))\n",
    "print('scp %s/%s %s. &&' % (SARS_DIR, identifier_uk + '.npy', SSH_HOME))\n",
    "print('scp %s/%s %s.'    % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)\n",
    "print('')\n",
    "\n",
    "print('scp -r %s%s %s' % (SSH_HOME, out_folder, SARS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding time series with good sampling in London\n",
    "input_dir     = os.path.join(SSH_DATA, 'data' + '-lond')\n",
    "output_dir    = input_dir\n",
    "script_file   = 'trim-sampling-intervals.py'\n",
    "trim_dir      = 'genome-trimmed'\n",
    "\n",
    "job_file      = 'job-trim-intervals-uk.sh'\n",
    "job_str       = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=200G\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/sampling-uk-error\n",
    "#SBATCH -o ./MPL/out/sampling-uk-out\n",
    "#SBATCH -p batch\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {script_file} --input_dir {input_dir} -o {output_dir} --freq_cutoff --trim_dir {trim_dir} --min_seqs 20 --window 7\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file), 'w')\n",
    "f.write('%s\\n' % job_str)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, script_file, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "print('sbatch %s' % job_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the data file in London\n",
    "# This cuts the data file to dates that have good sampling. The end point is arbitrary, but eliminating data before 2020-10-23 is important.\n",
    "# The \"days\" parameter in the bash script determines the number of days eliminated at the beginning of the time series.\n",
    "input_dir     = os.path.join(SSH_DATA, 'data' + '-lond', 'freq_0.05', 'europe-united kingdom-england-lond---2020-9-23-2021-5-27.npz')\n",
    "freq_script   = 'cut-file.py'\n",
    "out_file      = os.path.join(SSH_DATA, 'data' + '-lond', 'freq_0.05', 'europe-united kingdom-england-lond---2020-10-23-2021-5-27.npz')\n",
    "\n",
    "job_file1     = 'job-cut-series.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --time=4-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/cut-lond-error\n",
    "#SBATCH -o ./MPL/out/cut-lond-out\n",
    "#SBATCH -p batch\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "python {freq_script} --input_dir \\\"{input_dir}\\\" -o \\\"{out_file}\\\" --days 30\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "print(f'scp {SCRIPT_DIR}/{freq_script} {SSH_HOME} &&')\n",
    "print(f'scp {SCRIPT_DIR}/{job_file1} {SSH_HOME}')\n",
    "\n",
    "print('')\n",
    "print(f'sbatch {job_file1}')\n",
    "print(f'rm \"{input_dir}\"')\n",
    "\n",
    "print('')\n",
    "print('scp -r %s%s %s' % (SSH_HOME, output_dir, SARS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c979989156cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# infering the selection coefficients using a cutoff frequency of 5% in London\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_dir\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSH_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DATE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-lond'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'freq_0.05'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfreq_script\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m'epi-covar-parallel.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marchive_loc\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCRIPT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Archive-tv-int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemp_dir\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSH_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DATE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-lond'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'freqs-tv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# infering the selection coefficients using a cutoff frequency of 5% in London\n",
    "input_dir     = os.path.join(SSH_DATA, 'data' + '-lond', 'freq_0.05')\n",
    "freq_script   = 'epi-covar-parallel.py'\n",
    "archive_loc   = os.path.join(SCRIPT_DIR, 'Archive-tv-int')\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data' + '-lond', 'freqs-tv')\n",
    "\n",
    "# run on cluster\n",
    "print(f'mkdir {temp_dir}')\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "print('')\n",
    "\n",
    "num_files     = 3    # the number of data files that must be read from the directory\n",
    "\n",
    "job_file1     = 'job-find-freqs-tv-lond.sh'\n",
    "job_str1      = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --time=4-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/freq-tv-lond-error-%a\n",
    "#SBATCH -o ./MPL/out/freq-tv-lond-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "#SBATCH -p batch\n",
    "\n",
    "files=({input_dir}/*)\n",
    "shein\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "\n",
    "cd ./Archive-tv-int\n",
    "g++ src/main.cpp src/inf.cpp src/io.cpp -O3 -march=native -lgslcblas -lgsl -o bin/mpl -std=c++11\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python ../{freq_script} --data \\\"$file\\\" -o $tempout -q 5 --pop_size 10000 -k 0.1 -R 2 --scratch {SCRATCH} --timed 1 --tv_inference --window 15 --delta_t 15\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, freq_script, SSH_HOME))\n",
    "print('scp -r %s %s. &&'   % (archive_loc, SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster \n",
    "print('sbatch %s' % job_file1)\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print(f'scp -r {SSH_HOME}{temp_dir} {london_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_freqs = os.path.join(london_local, 'freqs-tv')\n",
    "t_start = []\n",
    "t_end   = []\n",
    "for file in os.listdir(local_freqs):\n",
    "    if os.path.isfile(os.path.join(local_freqs, file)):\n",
    "        times = np.load(os.path.join(local_freqs, file), allow_pickle=True)['times_full']\n",
    "        t_start.append(times[0])\n",
    "        t_end.append(times[-1])\n",
    "t_start = np.amin(t_start)\n",
    "t_end   = np.amax(t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_script    = 'epi-inf-parallel-tv.py'\n",
    "out_file      = os.path.join(SSH_DATA, f'infer-lond-tv-{DATA_DATE}')\n",
    "start_times   = np.arange(t_start,     t_end - 10, 10)   \n",
    "end_times     = np.arange(t_start + 9, t_end - 1,  10)\n",
    "\n",
    "s_times       = ' '.join([str(i) for i in start_times])\n",
    "e_times       = ' '.join([str(i) for i in end_times])\n",
    "print(s_times)    # copy paste this under s_times in the job_string\n",
    "print(e_times)    # copy paste this under e_times in the job_string\n",
    "\n",
    "\n",
    "job_file2 = 'job-infer-tv-lond.sh'\n",
    "job_str2  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/inf-tv-lond-error-%a\n",
    "#SBATCH -o ./MPL/out/inf-tv-lond-out-%a\n",
    "#SBATCH --array=0-17\n",
    "#SBATCH -p batch\n",
    "s_times=(300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450)\n",
    "e_times=(309 319 329 339 349 359 369 379 389 399 409 419 429 439 449 459)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "init_time=${{s_times[$SLURM_ARRAY_TASK_ID]}}\n",
    "final_time=${{e_times[$SLURM_ARRAY_TASK_ID]}}\n",
    "python {inf_script} --data {temp_dir} --timed 2 -o {out_file} -q 5 --g1 10 --start_time $init_time --end_time $final_time --delta_t 15\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file2), 'w')\n",
    "f.write('%s\\n' % job_str2)\n",
    "f.close()\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, inf_script, SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s.'      % (SCRIPT_DIR, job_file2, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print(f'mkdir {out_file}')\n",
    "print('sbatch %s' % job_file2)\n",
    "print(f'mv {out_file}---* {SSH_DATA}/infer-lond-tv-{DATA_DATE}')\n",
    "print('')\n",
    "\n",
    "# transfer files from cluster\n",
    "print('scp -r %s%s %s'      % (SSH_HOME, out_file, INF_DIR + '-lond'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14-lond/freq_0.05_new | wc -l\n",
      "scp /Users/brianlee/Python/MPL/epi-traj-parallel.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/data_processing.py blee098@cluster.hpcc.ucr.edu:. &&\n",
      "scp /Users/brianlee/SARS-CoV-2-Data/Processing-files/job-traj-lond.sh blee098@cluster.hpcc.ucr.edu:. &&\n",
      "\n",
      "sbatch job-traj-lond.sh\n",
      "\n",
      "scp -r blee098@cluster.hpcc.ucr.edu:/rhome/blee098/bigdata/SARS-CoV-2-Data/2021-08-14-lond/traj /Users/brianlee/SARS-CoV-2-Data/2021-08-14-lond\n"
     ]
    }
   ],
   "source": [
    "### TRAJECTORIES (unnormalized) ###\n",
    "\n",
    "input_dir     = os.path.join(SSH_DATA, 'data' + '-lond', 'freq_0.05')\n",
    "count_script  = 'epi-traj-parallel.py'\n",
    "temp_dir      = os.path.join(SSH_DATA, 'data' + '-lond', 'traj')\n",
    "\n",
    "# run on cluster to find number of files\n",
    "print(f'ls {input_dir} | wc -l')    # in order to find the number of files in the input directory\n",
    "num_files = 2    # the number of data files \n",
    "\n",
    "job_file1 = 'job-traj-lond.sh'\n",
    "job_str1  = f\"\"\"#!/bin/bash \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=100G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --mail-user={USER_NAME}{EMAIL_EXT}\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -e ./MPL/out/traj-lond-error-%a\n",
    "#SBATCH -o ./MPL/out/traj-lond-out-%a\n",
    "#SBATCH --array=0-{num_files - 1}\n",
    "#SBATCH -p batch\n",
    "\n",
    "files=({input_dir}/*)\n",
    "\n",
    "module purge\n",
    "module load anaconda3\n",
    "\n",
    "pwd\n",
    "\n",
    "file=${{files[$SLURM_ARRAY_TASK_ID]}}\n",
    "tempout={temp_dir}\n",
    "python {count_script} --data \\\"$file\\\" -o $tempout --timed 1 --window 15\n",
    "\"\"\" \n",
    "\n",
    "f = open(os.path.join(SCRIPT_DIR, job_file1), 'w')\n",
    "f.write('%s\\n' % job_str1)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# transfer files to cluster\n",
    "print('scp %s/%s %s. &&'   % (INF_SCRIPTS, 'epi-traj-parallel.py', SSH_HOME))\n",
    "print('scp %s %s. &&'      % (os.path.join(SCRIPT_DIR, 'data_processing.py'), SSH_HOME))\n",
    "print('scp %s/%s %s. &&'   % (SCRIPT_DIR, job_file1, SSH_HOME))\n",
    "print('')\n",
    "\n",
    "# run on cluster\n",
    "print('sbatch %s' % job_file1)\n",
    "print('')\n",
    "\n",
    "print(f'scp -r {SSH_HOME}{temp_dir} {london_local}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 1345)\n"
     ]
    }
   ],
   "source": [
    "# Combining inference files at different times into a single file\n",
    "alleles    = []\n",
    "selection  = []\n",
    "times_temp = []\n",
    "for file in os.listdir(os.path.join(INF_DIR + '-lond', f'infer-lond-tv-{DATA_DATE}')):\n",
    "    times_temp.append(int(file.split('---')[-1][:-4]))\n",
    "times = np.arange(np.amin(times_temp), np.amax(times_temp) + 1)\n",
    "\n",
    "for i in times:\n",
    "    data_temp = np.load(os.path.join(INF_DIR + '-lond', f'infer-lond-tv-{DATA_DATE}', f'infer-lond-tv-{DATA_DATE}---{i}.npz'))\n",
    "    if 'allele_number' in data_temp:\n",
    "        alleles = data_temp['allele_number']\n",
    "    selection.append(data_temp['selection'].flatten())\n",
    "np.savez_compressed(os.path.join(london_local, f'infer-lond-tv-{DATA_DATE}.npz'), selection=selection, allele_number=alleles, times=times)\n",
    "inf_times = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 1345)\n"
     ]
    }
   ],
   "source": [
    "# Combine trajectory files\n",
    "traj      = []\n",
    "muts      = []\n",
    "times     = []\n",
    "locs      = []\n",
    "t_full    = []\n",
    "traj_full = []\n",
    "traj_dir  = os.path.join(london_local, 'traj')\n",
    "for file in os.listdir(traj_dir):\n",
    "    data = np.load(os.path.join(traj_dir, file), allow_pickle=True)\n",
    "    times.append(data['times'])\n",
    "    muts.append(data['allele_number'])\n",
    "    traj.append(data['traj'])\n",
    "    locs.append(file[:-4])\n",
    "\n",
    "np.savez_compressed(os.path.join(london_local, 'trajectories.npz'),\n",
    "                    times=times, mutant_sites=muts, traj=traj, locations=locs, \n",
    "                    times_nosmooth=t_full, traj_nosmooth=traj_full)\n",
    "\n",
    "inf_data  = np.load(os.path.join(INF_DIR + '-lond', f'infer-lond-tv-{DATA_DATE}.npz'), allow_pickle=True)\n",
    "alleles   = inf_data['allele_number']\n",
    "selection = inf_data['selection']\n",
    "inf_times = inf_data['times']\n",
    "\n",
    "np.savez_compressed(os.path.join(INF_DIR + '-lond', f'infer-lond-tv-{DATA_DATE}-traj.npz'), allele_number=alleles,\n",
    "                    selection=selection, traj=traj, locations=locs, times=times, mutant_sites=muts,\n",
    "                    times_inf=inf_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha          = ['NSP6-106-0--', 'NSP6-106-1--', 'NSP6-106-2--', 'NSP6-107-0--', 'NSP6-107-1--', 'NSP6-107-2--', 'NSP6-108-0--',\n",
    "                  'NSP6-108-1--', 'NSP6-108-2--', 'S-501-0-T', 'NSP12-412-2-T', 'NSP2-36-2-T', 'NSP3-183-1-T', 'NSP3-890-1-A', \n",
    "                  'NSP3-1089-2-T', 'NSP3-1412-1-C', 'NSP12-613-2-T', 'NSP12-912-2-C', 'S-68-1--', 'S-68-2--', 'S-69-0--', 'S-69-1--', \n",
    "                  'S-69-2--', 'S-70-0--', 'S-143-2--', 'S-144-0--', 'S-144-1--', 'S-570-1-A', 'S-681-1-A', 'S-716-1-T', 'S-982-0-G', \n",
    "                  'S-1118-0-C', 'ORF8-27-0-T', 'ORF8-52-1-T', 'ORF8-73-1-G', 'N-3-0-C', 'N-3-1-T', 'N-3-2-A', 'N-235-1-T']\n",
    "tv_inf_file = os.path.join(INF_DIR, f'infer-lond-tv-{DATA_DATE}-traj.npz')\n",
    "out_file    = os.path.join(DATA_DIR, 'alpha-tv-s')\n",
    "dp.save_traj_selection(tv_inf_file, group=alpha, traj_site='S-570-1-A', out_file=out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
